\documentclass[final]{siamltex}
%test change
\usepackage{cite}
\usepackage{graphicx,bbm,pstricks}
\usepackage{pifont}
\usepackage{bbm,algorithmic,mdframed,placeins,multirow,booktabs,subfigure}
\usepackage[ruled,vlined,linesnumbered]{algorithm2e}

\setlength{\parindent}{0in}
\usepackage{amsmath,amsfonts,amsbsy,amssymb}
\newcommand{\RARR}[3]{#1
  \;\displaystyle\mathop{\displaystyle\longrightarrow}^{#3}\; #2}
\newcommand{\RARRlong}[3]{#1
  \;\displaystyle\mathop{-\!\!\!-\!\!\!-\!\!\!-\!\!\!-\!\!\!\!\displaystyle
  \longrightarrow}^{#3}\; #2}
\newcommand{\LARR}[3]{#1
  \;\displaystyle\mathop{\displaystyle\longleftarrow}^{#3}\; #2}
\newcommand{\LRARR}[4]{{\mbox{ \raise 0.4 mm \hbox{$#1$}}} \;
  \mathop{\stackrel{\displaystyle\longrightarrow}\longleftarrow}^{#3}_{#4}
  \; {\mbox{\raise 0.4 mm\hbox{$#2$}}}}
\newcommand{\bX}{{\bf X}}
\newcommand{\vecx}{{\mathbf x}}
\newcommand{\vecy}{{\mathbf y}}
\newcommand{\vecz}{{\mathbf z}}
\newcommand{\vecq}{{\mathbf q}}
\newcommand{\bs}{{\mathbf s}}
\newcommand{\vecr}{{\mathbf r}}
\newcommand{\vecX}{{\mathbf X}}
\newcommand{\vecv}{{\mathbf v}}
\newcommand{\tick}{\ding{52}}
\newcommand{\cross}{\ding{54}}
\newcommand{\vecn}{{\mathbf n}}
\newcommand{\vecp}{{\mathbf p}}
\newcommand{\cT}{{\mathcal T}}
\newcommand{\dt}{{\mbox{d}t}}
\newcommand{\dx}{{\mbox{d} \vecx}}
\newcommand{\boldnu}{{\boldsymbol \nu}}
\newcommand{\er}{{\mathbb R}}
\renewcommand{\div}{{\rm div}}
\newcommand{\bnu}{{\bf \nu}}
\newcommand{\divergence}{\mathop{\mbox{div}}}
\renewcommand{\P}{\mathbb{P}}
\newcommand{\FP}{P_{\rm{FP}}}
\newcommand{\ME}{P_{\rm{ME}}}
\newcommand{\MEs}{P_{\rm{ME}_{S}}}
\newcommand{\D}{\mathcal{D}}
\newcommand{\G}{\mathcal{G}}
\newcommand{\N}{\mathcal{N}}
\newcommand{\X}{{\mathbf X}}
\newcommand{\Y}{{\mathbf Y}}
\newcommand{\W}{{\mathbf W}}
\newcommand{\data}{D}
\newcommand{\neff}{n_{\text{eff}}}
\newcommand{\E}{{\mathbb E}}
\renewcommand{\b}[1]{{\bf #1}}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}

\newcommand{\picturesAB}[6]{
\centerline{
\hskip #4
\raise #3 \hbox{\raise 0.9mm \hbox{(a)}}
\hskip #5
\epsfig{file=#1,height=#3}
\hskip #6
\raise #3 \hbox{\raise 0.9mm \hbox{(b)}}
\hskip #5
\epsfig{file=#2,height=#3}
}}
\newcommand{\picturesCD}[6]{
\centerline{
\hskip #4
\raise #3 \hbox{\raise 0.9mm \hbox{(c)}}
\hskip #5
\epsfig{file=#1,height=#3}
\hskip #6
\raise #3 \hbox{\raise 0.9mm \hbox{(d)}}
\hskip #5
\epsfig{file=#2,height=#3}
}}

\makeatletter  
\newcommand{\xleftrightarrows}[2][]{\mathrel{%  
 \raise.40ex\hbox{$  
       \ext@arrow 3095\leftarrowfill@{\phantom{#1}}{#2}$}%  
 \setbox0=\hbox{$\ext@arrow 0359\rightarrowfill@{#1}{\phantom{#2}}$}%  
 \kern-\wd0 \lower.4ex\box0}}  
 
\newcommand{\xrightleftarrows}[2][]{\mathrel{%  
 \raise.40ex\hbox{$\ext@arrow 3095\rightarrowfill@{\phantom{#1}}{#2}$}%  
 \setbox0=\hbox{$\ext@arrow 0359\leftarrowfill@{#1}{\phantom{#2}}$}%  
 \kern-\wd0 \lower.4ex\box0}}  
 
\def\leftrightarrowfill@{%
 \arrowfill@\leftarrow\relbar\rightarrow%
 }  
\makeatother 

\author{Colin Cotter\thanks{Department of Mathematics, Imperial
    College, London, UK} \and Simon Cotter\thanks{School of
    Mathematics, University of Manchester, Manchester, UK. e:
    simon.cotter@manchester.ac.uk. SLC is grateful for EPSRC First
    grant award EP/L023393/1} \and Paul Russell\thanks{School of
    Mathematics, University of Manchester, Manchester, UK}}
\title{Parallel Adaptive Importance Sampling}
\begin{document}
\maketitle
\begin{abstract}
  Markov chain Monte Carlo methods are a powerful and commonly used
  family of numerical methods for sampling from complex probability
  distributions. As applications of these methods increase in size and
  complexity, the need for efficient methods which can exploit the
  parallel architectures which are prevalent in high performance
  computing increases. In this paper, we aim to develop a framework
  for scalable parallel MCMC algorithms. At each iteration, an
  importance sampling proposal distribution is formed using the
  current states of all of the chains within an ensemble. Once
  weighted samples have been produced from this, a state-of-the-art
  resampling method is then used to create an evenly weighted sample
  ready for the next iteration. We demonstrate that this parallel
  adaptive importance sampling (PAIS) method outperforms naive
  parallelisation of serial MCMC methods using the same number of
  ensemble members, for low dimensional problems, and in fact shows
  better than linear improvements in convergence rates with respect to
  the number of ensemble members. We also introduce a new resampling
  strategy, approximate multinomial resampling (AMR), which while not
  as accurate as other schemes is substantially less costly for large
  ensemble sizes, which can then be used in conjunction with PAIS for
  complex problems. In particular, we demonstrate this methodology's
  superior sampling for multimodal problems, such as those arising
  from inference for mixture models.
\end{abstract}
\begin{keywords}MCMC, parallel, importance sampling, Bayesian, inverse problems.
\end{keywords}
\section{Introduction}
Having first been developed in the early 1970s\cite{hastings1970monte}, Markov chain Monte Carlo (MCMC) methods have been of increasing
importance and interest in the last 20 years or so. They allow us to
sample from complex probability distributions which we would not be
able to sample from directly. In particular, these methods have
revolutionised the way in which inverse problems can be tackled,
allowing full posterior sampling when using a Bayesian framework. 

However, this often comes at a very high cost, with a very large
number of iterations required in order for the empirical approximation
of the posterior to be considered good enough. As the cost of
computing likelihoods can be extremely large, this means that many
problems of interest are simply computationally intractable.

This problem has been tackled in a variety of different ways. One
approach is to constuct increasingly complex MCMC methods which are
able to use the structure of the posterior to make more intelligent
proposals, leading to more thorough exploration of the posterior with
fewer iterations. For example, the Hamiltonian or Hybrid Monte Carlo
(HMC) algorithm uses gradient information and simplectic integrators
in order to make very large moves in state with relatively high
acceptance probability\cite{sexton1992hamiltonian}. Non-reversible
methods are also becoming quite popular as they can improve
mixing\cite{bierkens2015non}. Riemann
manifold Monte Carlo methods exploit the Riemann geometry of the
parameter space, and are able to take advantage of the local structure
of the target density to produce more efficient MCMC
proposals\cite{girolami2011riemann}. This methodology has been
successfully applied to MALA-type proposals and methods which exploit
even higher order gradient information\cite{bui2014solving}.

Since the clock speed of an individual processor is no longer
following Moore's law\cite{moore1998cramming}, improvements in
computational power are largely coming from the parallelisation of
multiple cores. As such, the area of parallel MCMC methods is becoming
increasingly of interest. One class of parallel MCMC method uses
multiple proposals, with only one of these proposals being
accepted. Examples of this approach include multiple try
MCMC\cite{liu2000multiple} and ensemble MCMC\cite{neal2011mcmc}. In
\cite{calderhead2014general}, a general construction for the
parallelisation of MCMC methods was presented, which demonstrated
speed ups of up to two orders of magnitude when compared with serial
methods.

Other methods are also designed for particular scenarios. For
instance, the challenge of sampling from high/infinite-dimensional
posterior distributions is a big one. The majority of
Metropolis-Hastings algorithms suffer from the curse of
dimensionality, requiring more samples for a given degree of accuracy
as the dimension of problem is increased. However some
dimension-independent methods have been developed, based on
Crank-Nicolson discretisations of certain stochastic differential
equations\cite{cotter2013mcmc}. Other ideas such as chain
adaptation\cite{haario2005componentwise} and early rejection of
samples can also aid reduction of the computational
workload\cite{solonen2012efficient}.

However, high dimensionality is not the only challenge that we may
face. Complex structure in low
dimensions can cause significant issues. These issues may arise due to
large correlations between parameters in the posterior, leading to
long thin curved structures which many standard methods can struggle
with. These features are common, for example, in inverse problems
related to epidemiology and
other biological applications\cite{house2016bayesian}. Multimodality of the posterior can also lead to
incredibly slow convergence in many methods. Many methods allow for
good exploration of the current mode, but the waiting time to the next
switch of the chain to another mode may be large. Since many switches
are required in order for the correct weighting to be given to each
mode, and for all of the modes to be explored fully, this presents a
significant challenge.

One application where this is an ever-present problem is that of
mixture models. Given a dataset, where we know that the data is from
two or more different distributions, we wish to be able to identify
the parameters, e.g. the mean and variance and relative weight, of
each part of the mixture\cite{marin2005bayesian}. The resulting posterior
distribution is invariably a multimodal distribution, since it is
usually invariant to permutations. Metropolis-Hastings algorithms, for
example, will often fail to converge in a reasonable time frame for
problems such as this. Since the posterior may be multimodal,
independent of this label switching, it is still important to be able
to efficiently sample from the whole posterior.

Importance sampling methods are another class of methods which allow
us to sample from complex probability distribution. A related class of algorithms, adaptive importance sampling (AIS)
\cite{liu2008monte} reviewed in\cite{bugallo2015adaptive}, had
received less attention until their practical applicability was
demonstrated in the
mid-2000s\cite{celeux2006iterated,cappepopulation,isard1998condensation,bink2008bayesian}.
AIS methods produce a sequence of approximating distributions,
constructed from standard distributions, from which samples can be
easily drawn. At each iteration the samples are weighted, often using
standard importance sampling methods. The weighted samples are used to
train the adapting sequence of distributions so that samples are drawn
more efficiently as the iterations progress. The weighted samples form
a sample from the posterior distribution under some mild
conditions~\cite{robert2013monte,martino2015adaptive}.

Ensemble importance sampling schemes also exist, such as population Monte Carlo (PMC)~\cite{cappe2012population}. This algorithm uses a population of
samples to build a mixture or kernel density estimate (KDE) of the
posterior distribution. The efficiency of
this optimisation is restricted by the component kernel(s) chosen, and
the quality of the current sample from the posterior. Extensions, such
as adaptive multiple importance sampling algorithm
(AMIS)~\cite{cornuet2012adaptive}, and adaptive population importance sampling
(APIS)~\cite{martino2015adaptive} have allowed these methods to
applied to various applications, including population
genetics~\cite{siren2011reconstructing}.

In this paper, we present a framework for parallelisation of
importance sampling, which can be built around many of the current
Metropolis-based methodologies in order to create an efficient target
proposal from the current state of all of the chains in the
ensemble. The method makes use of a resampler based on optimal
transport which has been used in the context of particle
filters\cite{reich2013nonparametric}. In particular we demonstrate the
advantages of this method when attempting to sample from multimodal
posterior distributions, such as those arising from inference for
mixture models.

In Section \ref{Sec:Prelim} we introduce some mathematical
preliminaries upon which we will later rely. In Section \ref{Sec:PAIS}
we present the general framework of the PAIS algorithm. In Section
\ref{Sec:adapt} we consider adaptive versions of PAIS which
automatically tune algorithmic parameters concerned with the proposal
distributions. In Section \ref{sec:AMR} we introduce the approximate
multinomial resampling (AMR) algorithm which is a less accurate but
faster alternative to resamplers which solve the optimal transport
problem exactly. In Section~\ref{sec:consistency} we consider
consistency of the PAIS algorithm. In Section \ref{Sec:Num} we present
some numerical examples, before a brief conclusion and discussion in
Section \ref{Sec:Conc}.

\section{Preliminaries}\label{Sec:Prelim}
In this Section we will introduce preliminary topics and algorithms
that will be referred to throughout the paper.
\subsection{Bayesian inverse problems}
In this paper, we focus on the use of MCMC methods for characterising
posterior probability distributions arising from Bayesian inverse problems. We
wish to learn about a particular unknown quantity $u$, of which we are
able to make direct or indirect noisy observations. For now
we say that $x$ is a member of a Hilbert
space $X$. 

The parameter $x$ is observed
through the observation operator $\mathcal{G}:X \to\mathbb{R}^d$. 
Since observations are never perfect, we
assume that these measurements $\data$ are subject to Gaussian noise,
so that
\begin{equation}\label{eqn:obs}
	\data = \mathcal{G}(x) + \varepsilon, \qquad \varepsilon \sim \mu_{\varepsilon} = \mathcal{N}(0,\Sigma).
\end{equation}
For example, if $x$ are the rates of reactions in a chemical system, $\G$ might
return the quantities of each chemical species at a particular time, or some
summary of this information.

These modelling assumptions allow us to construct the 
likelihood of observing the data $\data$ given the parameter $x =
x^*$. Rearranging \eqref{eqn:obs} and using the distribution of
$\varepsilon$, we get:
\begin{equation}\label{eqn:like}
	\mathbb{P}(\data|x=x^*) \propto \exp \left ( -\frac{1}{2} \|\mathcal{G}(x^*)
	  - \data\|_\Sigma^2 \right ) = \exp\left(-\Phi(x^*)\right),
\end{equation}
where $\| y_1 - y_2 \|_\Sigma$ is the Mahalanobis distance between $y_1$ and $y_2 \in \mathbb{R}^d$.

As discussed in \cite{stuart2010inverse,cotter2009bayesian},
in order for this inverse problem to be well-posed in the Bayesian
sense, we require the posterior distribution, $\mu_Y$, to be absolutely
continuous with respect to the prior, $\mu_0$. A
minimal regularity prior can be chosen informed by regularity results
of the observational operator $\mathcal{G}$. Given such a prior, then
the Radon-Nikodym derivative of the posterior measure, $\mu_Y$, with
respect to the prior measure, $\mu_0$, is proportional to the
likelihood:
\begin{equation}\label{eqn:RND}
	\frac{d\mu_Y}{d\mu_0} \propto \exp \left ( -\Phi(x^*) \right ).
\end{equation}

% \subsection{The preconditioned Crank-Nicolson Langevin (pCNL) algorithm}\label{Sec:pCNL}
% In recent years, work has been carried out to frame MCMC proposal
% distributions on function space\cite{cotter2013mcmc}. These new
% discretisations perform comparably with the original versions in low
% dimensions. If gradient information regarding the observation operator is
% available, then a range of MCMC methods are available which exploit this information to improve mixing rates. One
% example of such an algorithm is MALA. In \cite{cotter2013mcmc}, a function space version of this
% method was presented, the pCNL algorithm, and is described in full in Table
% \ref{tab:pCNL}. The proposal used in this method comes about through
% a Crank-Nicolson approximation of the Langevin SDE, whose invariant
% measure is the posterior measure $\mu_Y$.

% \begin{table}
% \begin{mdframed}
% \begin{algorithmic}
% \STATE $X = x_0$
% \FOR{$i=1,2,3,\ldots$}
% \STATE $Y = (2+\delta)^{-1}\left[(2 - \delta)X_{i-1}-
% 2\delta\mathcal{C}\nabla \Phi(u)+
% \sqrt{8\delta} W\right] $, $W \sim \mu_0$
% \STATE $a(X_{i-1},Y) = \min \left \{ 1,  \exp(\Phi(X_{i-1}) - \Phi(Y) ) \right \}$.
% \STATE $u \sim U([0,1])$
% \IF{$u<a(X_{i-1},Y)$}
% \STATE $X_i = Y$
% \ELSE 
% \STATE $X_i = X_{i-1}$
% \ENDIF
% \ENDFOR
% \end{algorithmic}
% \end{mdframed}\caption{A pseudo-code representation of the preconditioned Crank-Nicolson Langevin
%    (pCNL) algorithm. $\delta \in (0,2]$ is a step size parameter.}
% \label{tab:pCNL}
% \end{table}



\subsection{Particle filters and resamplers}\label{sec:filters}
In several applications, data must be assimilated in an ``online''
fashion, with up to date observations of the studied system being made
available on a regular basis. In these contexts, such as in weather forecasting or
oceanography, data is incorporated using a filtering
methodology. One popular filtering method is the particle filter, the
first of which was dubbed the Bootstrap
filter\cite{gordon1993novel}. In this method, a set of weighted particles is
used to represent the posterior distribution. The positions of the
particles are updated using the model dynamics. Then, when more
observations are made available, the relative weights of the particles
are updated to take account of this data, using Bayes' formula. Other filtering methods, such
as the Kalman filter\cite{kalman1960new} and ensemble Kalman filter\cite{evensen1994sequential}, have also been developed which are often used within the data
assimilation community.

One advantage of the particle filter is that there are convergence
results for this method as the number of particles is increased.  One downside is that the required ensemble
size increases quickly with dimension, making it difficult to use in
high-dimensional problems. Another
downside is that the effective sample size decreases at each
iteration, resulting in degeneration of the approximation of the
posterior. One way to tackle
this is to employ a resampling scheme. The aim of a successful
resample is to take the unevenly weighted ensemble and return a new
ensemble of particles with even weights which is highly correlated to
the original samples.

The ensemble transform particle filter (ETPF) proposed by Reich
\cite{reich2013nonparametric} makes use of optimal transportation as
described in \cite{villani2003topics,villani2008optimal}. The transform
takes a sample of weighted particles $\{y_i\}_{i=1}^M$ from $\mu_Y$
and converts it into a sample of evenly weighted particles $\{x_i\}_{i=1}^M$
from $\mu_X$, by means of defining a coupling $T^*$ between $Y$ and
$X$. Given that a trivial coupling $T^t$ always exists in the space of
transference plans, $\Pi(\mu_{X}, \mu_{Y})$, we can find a coupling
$T^*$ which maximises the correlation between $X$ and $Y$
\cite{cotter2012ensemble}. This coupling is the solution to a linear
programming problem in $M^2$ variables with $2M-1$ constraints.
Maximising the correlation ensures that the new sample is as much like
the original sample as possible with the additional property that the sample
is evenly weighted.

A Monte Carlo algorithm can be implemented to resample from a weighted
ensemble. We create a weighted sample, then solve the optimal
transport problem which produces the coupling described above, we can
draw a new sample from the evenly weighted distribution. Reich
suggests using the mean of the evenly weighted distribution to produce
a consistent estimator. Analysis of this method shows that as the
ensemble size increases, the statistics of the evenly weighted sample
approach those of the posterior distribution.

\subsection{Deficiencies of Metropolis-type MCMC schemes}
All MCMC methods are naively parallelisable. One can take a method
and simply implement it simultaneously over a set of processors in an ensemble. All
of the states of all of the ensemble member can be recorded, and in the time
that it takes one MCMC chain to draw $N$ samples, $M$ ensemble members
can draw $NM$ samples. 

However, we argue that this is not an optimal
scenario. First of all, unless we have a lot of information about the
posterior, we will initialise the algorithm's initial state in the
tails of the distribution. The samples that are initially made as the
algorithm finds its way to the mode(s) of the distribution cannot be
considered to be samples from the target distribution, and must be thrown
away. This process is known as the burn-in. In a naively parallelised
scenario, each ensemble member must perform this process independently,
and therefore mass parallelisation makes no inroads to cutting this cost.

Moreover, many MCMC algorithms suffer from poor mixing, especially in
multimodal systems. The amount of samples that it takes for an MCMC
trajectory to switch between modes can be large, and given that a large
number of switches are required before we have a good idea of the relative
probability densities of these different regions, it can be prohibitively
expensive.

Another aspect of Metropolis-type samplers is that information
computed about a proposed state is simply lost if we choose to reject
that proposal in the Metropolis step. An advantage of importance
samplers is that no evaluations of $\mathcal{G}$ are ever wasted
since all samples are saved along with their relative weighting.

Moreover, a naively parallelised MCMC scheme is exactly that -
naive. Intuition suggests that we can gain some speed up by sharing
information across the ensemble members, and this is
what we wish to demonstrate in this paper.

These deficiencies of the naive method of parallelising MCMC methods
motivated the development of the Parallel Adaptive Importance Sampler
(PAIS). In the next section we will introduce the method in its most
general form.


\section{The Parallel Adaptive Importance Sampler \allowbreak (PAIS)}\label{Sec:PAIS}

% In a trivially parallelised MCMC scheme, if we have enough processes
% all sampling from the target density at the same time, then we can
% make a very rough approximation of that density by taking the current
% states of all of the processes as a representative sample. In the
% PAIS, the proposal density for the importance sampler stage is a
% mixture of all of the proposal densities for a chosen MCMC scheme,
% using the current states. If the proposal in the MCMC scheme is
% appropriate and our number of samples is big enough, we might hope
% that this mixture of densities is a good enough approximation of the
% target density, and that new samples drawn from
% the mixture proposal are approximately draws from the target
% measure. If this is the case, then the variance of importance weights
% for these samples will be small, leading to a highly efficient
% sampler.

% The problem at this stage, is that our current states all have varying
% importance, but in order for us to iterate this whole process again,
% we require them to be the same. Therefore, we employ a resampler to
% take our new set of samples with differing weights, and return a set
% of states with equal weights, which are chosen so that the correlation
% between the two sets is maximised. At this stage we can start the whole
% process again. 

Importance sampling can be a very efficient method for sampling from a
probability distribution. A proposal density is chosen, from which we
can draw samples. Each sample is assigned a weight given by the
ratio of the target density and the proposal density at that
point. They are efficient when the proposal density is concentrated in
similar areas to the target density, and incredibly inefficient when
this is not the case. The aim of the PAIS is to use an ensemble of states,
each coming from a MCMC chain, to construct a proposal
distribution which will be as close as possible to the target density. If this
ensemble is large enough, the distribution of states will be representative
of the target density.

The proposal distribution could be constructed in many different ways,
but we choose to use a mixture distribution, made up of a sum of MCMC
proposal distributions for each of the members of the ensemble. Once the proposal is
constructed, we can sample a new set of states from the proposal
distribution, and each is assigned a weight given by the ratio of the target
density and the proposal mixture distribution density. Assuming that
our proposal distribution is a good one, then the variance of the
weights will be small, and we will have many useful samples. Finally, we
need to create a set of evenly weighted samples which best represent
this set of weighted samples. This is achieved by implementing a
resampling algorithm. Initially we will use the ETPF
algorithm\cite{reich2013nonparametric}, although we will suggest an
alternative strategy in Section \ref{sec:AMR}. The resampling
algorithm gives us a set of evenly weighted samples which represents
the target distribution well, which we can use to iterate the process
again. The algorithm is summarised in Algorithm \ref{alg:PAIS}. 

We wish to sample states $x \in X$ from a posterior
probability distribution $\mu_D$, where $D$ represents our data which
we wish to use to infer $x$. Since we have $M$ ensemble members, we
represent the current state of all of the Markov chains as a vector
$\X = [x_1,x_2,\ldots,x_M]^T$. We are also given a transition kernel
$\nu(\cdot,\cdot)$, which might come from an MCMC method, for example
the random walk Metropolis-Hastings proposal density $\nu(\cdot,x) \sim
\mathcal{N}(x,\beta^2)$, where $\beta^2\in \mathbb{R}$ defines the
variance of the proposal.

%\begin{table}
%\begin{mdframed}
%\begin{algorithmic}
%\STATE $\X^{(0)} = \X_0 = [x_1^{(0)},x_2^{(0)},\ldots,x_M^{(0)}]^T$
%\FOR{$i=0,1,2, ..., N-1$}
%\STATE $\Y^{(i)} = [y_1^{(i)},y_2^{(i)},\ldots,y_M^{(i)}]^T, \quad y_j^{(i)} \sim
%\nu(\cdot;x_j^{(i)})$
%\STATE $\chi(y;\X^{(i)}) = \frac{1}{M}
%\sum_{j=1}^M \nu(y;x_j^{(i)})$.
%\STATE $\W^{(i)} = [w_1^{(i)},w_2^{(i)},\ldots,w_M^{(i)}]^T,$ \quad $w^{(i)}_j =
%\frac{\pi(y_j^{(i)})}{\chi(y_j^{(i)};\X^{(i)})}$.
%\STATE Resample: $(\W^{(i)},\Y^{(i)}) \rightarrow (\frac{1}{M}\mathbf{1}, \X^{(i+1)})$
%\ENDFOR 
%\end{algorithmic}
%\end{mdframed}
%\caption{A pseudo-code representation of the Parallel Adaptive
%  Importance Sampler (PAIS).}
%\label{tab:PAIS}
%\end{table}

\begin{table}[!h]
\centering
\begin{algorithm}[H]
\DontPrintSemicolon
\BlankLine
	Set $\X^{(0)} = \X_0 = [x_1^{(0)},x_2^{(0)},\ldots,x_M^{(0)}]^\top$.\;
	\For {$i = 1, \dots, N$}
	{
		Sample $\Y^{(i)} = [y_1^{(i)},y_2^{(i)},\ldots,y_M^{(i)}]^\top, \quad y_j^{(i)} \sim
\nu(\cdot;x_j^{(i-1)})$.\;
		Calculate $\W^{(i)} = [w_1^{(i)},w_2^{(i)},\ldots,w_M^{(i)}]^\top,$ \quad $w^{(i)}_j =
\frac{\pi(y_j^{(i)})}{\chi(y_j^{(i)};\X^{(i-1)})}$, where
		\[
			\chi(y;\X^{(i-1)}) = \frac{1}{M}\sum_{j=1}^M \nu(y;x_j^{(i-1)}).
		\]

		Resample: $(\W^{(i)},\Y^{(i)}) \rightarrow (\frac{1}{M}\mathbf{1}, \X^{(i)})$.
	}
	Output $(\W, \Y)$.\;
\caption{The parallel adaptive importance sampler (PAIS).\label{alg:PAIS}}
\end{algorithm}
\end{table}

Since the resampling does not give us a statistically identical sample
to that which is inputted, we cannot assume that the samples $\X^{(i)}$
are samples from the posterior. Therefore, as with serial
importance samplers, the weighted samples
$(\W^{(i)},\Y^{(i)})_{i=1}^N$ are the samples from the posterior that
we will analyse.

The key is to choose a suitable transition kernel $\nu$ such that
if $X^{(i)}$ is a representative sample of the posterior,
then the mixture density $\chi(\cdot;\X^{(i)})$ is a good
approximation of the posterior distribution. If this is the case,
the newly proposed states $\Y^{(i)}$ will also be a good sample of the posterior with low variance in the
weights $\W^{(i)}$.

In Section \ref{Sec:Num}, we will demonstrate how the algorithm
performs, primarily using RWMH proposals. We do not claim that this choice is optimal, but
is simply chosen as an example to show that sharing information across
ensemble members can improve on the original MCMC algorithm and lead to
convergence in fewer evaluations of $\G$. This is important since if
the inverse problem being tackled involves computing
the likelihood from a very large data set this could lead to a
large saving of computational cost. We have observed that
using more complex (and hence more expensive) kernels $\nu$, does not
significantly improve the speed of convergence of the algorithm for
simple examples\cite{Paul}.

Care needs to be taken when choosing the proposal distribution to
ensure that the proposals are absolutely continuous with respect to
the posterior distribution. In Section \ref{sec:chem} we will consider
an inverse problem with Gamma priors. Since the posterior distribution
in this context is heavy-tailed, if we use a lighter tailed
distribution for $\nu$, such as a Gaussian kernel, then importance
weights in the tails will be unbounded, which will hamper convergence
of the algorithm.

\section{Automated tuning of Algorithm Parameters}\label{Sec:adapt}

Efficient selection of scaling parameters in MCMC algorithms is
critical to achieving optimal mixing rates and hence achieving fast
convergence to the target density. It is well known that a scaling
parameter which is either too large or too small results in a Markov
chain with high autocorrelation. One aspect worthy of consideration
with the PAIS, is finding an appropriate proposal kernel $\nu$ such
that the mixture distribution $\chi$ is a close approximation to the
posterior density $\pi$.

%If the
%proposal distribution is too over-dispersed, then the algorithm will often
%propose states in the tails of the distribution, resulting in larger
%variance of the weights, and therefore slower convergence to the
%posterior distribution. Similarly, if the proposal distribution is
%under-dispersed, the proposals will be highly correlated with the
%previous states, and the algorithm will take a long time to fully explore
%the parameter space, and worse, will lead regularly to states proposed
%in the tails of the proposal distribution with very large weights. It is therefore necessary to find a proposal
%distribution which is slightly over-dispersed to ensure the entire
%posterior is explored\cite{gelman1992inference}, but is as close to the
%posterior as possible.

Most commonly used MCMC proposals have parametric dependence which
allows the user to control their variance. For example, in the RWMH
proposal $y = x + \beta \eta$, the parameter $\beta$ is the variance
of the proposal distribution. Therefore the proposal distributions can
be tuned such that they are are slightly over-dispersed. This tuning
can take place during the burn-in phase of the algorithm. Algorithms
which use this method to find optimal proposal distributions are known
as adaptive MCMC algorithms, and have been shown to be convergent
provided that they satisfy certain
conditions\cite{roberts2007coupling,roberts2009examples}.
Alternatively, a costly trial and error scheme with short runs of the
MCMC algorithm can be used to find an acceptable value of $\beta$.

Algorithms which use mixture proposals, e.g. PAIS, must tune the
variance of the individual kernels within the proposal mixture. This
adaptivity during early iterations has some added benefits over and above
finding an optimal parameter regime for the algorithm. If the initial
value of the proposal variance is chosen to be very large, then the early
mode-finding stages of the algorithm are expedited. Adaptively
reducing the proposal variances to an optimal value then allows us to
explore each region efficiently. Using an ensemble of chains allows
quick and effective assessment of the value of the optimal scaling
parameter.

%The alternative to using adaptive procedures to tune the scaling
%parameters is to perform exploratory simulations to find the optimal
%regimes by trial and error. This can be very costly and the optimal
%parameters cannot realistically be found to more than a couple of
%significant figures. It is therefore important that MCMC algorithms
%provide a feasible adaptive strategy.

In many MCMC algorithms such as the Random Walk Metropolis-Hastings
(RWMH) algorithm, the optimal scaling parameter can be found by
searching for the parameter value which gives an optimal acceptance
rate, e.g. for near Gaussian targets the optimal rates are 23.4\% for
RWMH and 57.4\% for MALA\cite{roberts2001optimal}. This method
is not applicable to PAIS so we must use other statistics to optimise
the scaling parameter. Section~\ref{sec:statistics} gives some
possible methods for tuning $\beta$.

\subsection{Statistics for Determining the Optimal Scaling Parameter}\label{sec:statistics}

\subsubsection{Determining optimal scaling parameter using error analysis}

%Sampling algorithms can be assessed by comparing their approximation of
%the posterior to the analytic distribution, in cases where the
%posterior distribution can be computed using alternative methods. 

When available, an analytic form for the target distribution allows us
to assess the convergence of sampling algorithms to the target
distribution. Common metrics for this task include the relative error
between the sample moments and the target's moments, or the
relative $L^2$ error between the sample histogram and the target
density, $\pi(x|D)$. The relative error in the $n$-th moment,
$\hat{m}_n$, is given by:
\begin{equation}\label{eq:34567}
	e = \left|\frac{\hat{m}_n - \mathbb{E}[X^n]}{\mathbb{E}[X^n]}\right|, \quad \text{where} \quad \hat{m}_n = \frac{1}{N}\sum_{i=1}^N \! x_i^n,
\end{equation}
where $\{x_i\}_{i=1}^N$ is a sample of size $N$.

The relative $L^2$ error, $E$, between a continuous function to a
piecewise constant function, can be given by considering the
difference in mass between the self-normalised histogram of the
samples and the posterior distribution over a set of disjoint sets or
``bins'':
\begin{equation}\label{eqn:L2_error}
	E^2 = \sum\limits_{i=1}^{n_b}\left[\displaystyle\int_{R_i} \! \pi(s|D) \, \mbox{d}s - vB_i\right]^2 \Big/ \sum\limits_{i=1}^{n_b}\left[\displaystyle\int_{R_i} \! \pi(s|D) \, \mbox{d}s\right]^2,
\end{equation}
where the regions $\{R_i\}_{i=1}^{n_b}$ are the $d$-dimensional
histogram bins, so that $\bigcup_i R_i \subseteq X$ and
$R_i\cap R_j=\emptyset$, $n_b$ is the number of bins, $v$ is the
volume of each bin, and $B_i$ is the value of the $i$th bin. This
metric converges to the standard definition of the relative $L^2$
error as $v\rightarrow 0$.

These statistics cannot be used in general to find optimal values of
$\beta$ since they require knowledge of the analytic solution, and
the algorithm must be run for a long time to build up a
sufficiently large sample. However they can be used to assess the
ability of other indicators to find the optimal scaling parameters in
a controlled setting.

%\subsubsection{The variance of the weights}

%Importance samplers assign a weight to each sample they produce based
%on a ratio of the posterior to the proposal at that point. Importance
%samplers are most efficient when the target is proportional to the
%proposal distribution. In this case the weights are all equal, and so the variance of the weights, $var(w(y))$, is zero. Hence, we would like to %choose the value of $\beta$ which minimises the variance of the weights,
%\[
%	\beta^*_{\text{var}} = \argmin_{\beta} \text{var}(w(y)).
%\]
%In our experience, the mean of the estimator
%$\text{var}(w(y))$ is a smooth enough function of $\beta$ that it can
%be used to tune the proposal variance during the burn-in phase of MCMC
%algorithms. However the variance of the estimator of the variance can be large, especially far away from the optimal value, so it can take a %large number of iterations to calculate descent directions.

\subsubsection{The effective sample size}\label{sec:ess}

The effective sample size, $\neff$, can be used to assess the
efficiency of importance samplers. Ideally, in each iteration, we
would like all $M$ of our samples to provide us with new information
about the posterior distribution. In practise, we cannot achieve a
perfect effective sample size of $M$.

The effective sample size of a weighted sample is defined in the
following way:
\[
	\neff = \frac{\left(\sum_{i=1}^M \! w_i\right)^2}{\sum_{i=1}^M \! w_i^2} \approx \frac{M\E(w)^2}{\E(w^2)} = M\left(1 - \frac{\mbox{var}(w)}{\mathbb{E}(w^2)}\right).
\]
The second two expressions are true when $M\rightarrow\infty$. From
the last expression, if the variance of the weights is zero then
$\neff = M$; this is our ideal scenario. Maximising the effective
sample size is equivalent to minimising the variance of the weights.

The statistic $\neff$ is easier to deal with than the variance of the
weights, as it takes values on $[1, M]$ while the variance of the
weights takes values on $\mathbb{R}_\geq$. Since the variance can vary
over many orders of magnitude bringing numerical issues, the effective
sample size is more desirable.

%In the numerics which follow, we use the effective sample size
%calculated at each iteration and averaged across the entire run to
%tune the scaling parameters. We do this because when we tune this
%parameter on the fly, we use the single-iteration average to estimate
%optimality. The optimal scaling parameter found using the global
%optimum of the variance of the weights is also included for
%comparison. Note that for the majority of iterations the value of
%$\neff$ is an overestimate of this statistic over a larger number of
%samples. Rare events which result in a large importance weight bring
%this statistic down, and care must be made not to overfit the proposal
%variance to $\neff$ on an iteration by iteration basis. This can be
%accounted for by increasing the variance slightly once the adaptive
%algorithm has arrived on a value using single iteration values of $\neff$.

In this paper we tune our simulations using the effective sample size
statistic. We calculate this statistic using a sample size of $Mn_k$,
where $1 \leq n_k\leq N$ is sufficiently large enough to obtain a
reasonable estimate of $\neff$ with scaling parameter $\delta_k$.
Calculating $\neff$ over these subsets of the simulations tends to
underestimate the optimal value of the scaling parameter due to the
possibility of missing unlikely proposals with extreme weights.

The effective sample size also has another useful property; if we
consider the algorithm in the early stages, for example, we have all
$M$ particles in the tails of the target searching for a mode. The
particle closest to the mode will have an exponentially higher weight
and the effective sample size will be close to 1. In later stages the
ensemble populates high density regions, and better represents the
posterior distribution. This leads to smaller variation in the
weights, and a higher effective sample size. By looking for
approximations of the effective sample size which look like a
stationary distribution, we can tell when the algorithm is working
efficiently.

\begin{figure}[htb]
\centering
\includegraphics[width=0.65\textwidth]{"figures/C1_burnin"}
\caption{The effective sample size ratio and relative $L^2$ difference
$E$ during the first 30 iterations. These numerics are taken from the
example in Section~\ref{sec:chem} using the PAIS-Gamma algorithm.}
\label{fig:neff-burnin}
\end{figure}

Figure~\ref{fig:neff-burnin} demonstrates that the effective sample
size flattens out as the relative $L^2$ difference between the
posterior distribution and the proposal distribution stabilises close
to its minimum.

\begin{figure}[htb]
\centering
\subfigure[Contours showing optimal ranges of the scaling parameter.]{\includegraphics[width=0.47\textwidth]{"figures/ESSvM_contour"}}
\subfigure[The highest effective sample size achieved for each ensemble size.]{\includegraphics[width=0.47\textwidth]{"figures/ESSvM"}}
\caption{The behaviour of the effective sample size as the ensemble size increases. This analysis is of the example in Section~\ref{sec:problem 1} using the PAIS-RW algorithm.}
\label{fig:neff-M}
\end{figure}

Figure~\ref{fig:neff-M} shows how the effective sample size used in
PAIS is affected by the ensemble size. We see from subfigure (a) that
as the ensemble size increases, the optimal scaling parameter
decreases. This is expected since the scaling parameter must be
reduced if we are to fit more kernels into the same target. We also
see that the algorithm becomes less sensitive to changes in the
scaling parameter as the ensemble size increases. Subfigure (b) shows
that as the ensemble size increases, the algorithm becomes more
efficient.

%If we are to use the effective sample size ratio as an indicator of
%how to tune the proposal variance, we need to look at how it behaves
%in different situations. Here we look at how the statistic behaves as
%we vary the ensemble size, $M$. Figure~\ref{fig:neff-M} shows results
%for the Gaussian posterior discussed in Section~\ref{sec:problem 1}
%when using the PAIS algorithm with RW proposals, each with variance
%$\beta^2 \in \mathbb{R}_{>0}$. Figure~\ref{fig:neff-M} (a) shows that
%as the ensemble size increases, the scaling parameter which gives the
%optimal effective sample size ratio decreases. This is to be expected
%since if we are trying to approximate the posterior with a mixture
%distribution of a small number of Gaussians, the optimal variance will
%naturally be larger so that the whole of the significant regions are
%covered by the proposal distribution. As the number increases, the
%optimal variances decrease so that finer details in the posterior can
%be better represented in the proposal. Figure~\ref{fig:neff-M} (b) shows that as the ensemble size increases, the efficiency of the sampler also increases.

\subsection{Adaptive PAIS}\label{sec:adapt}

%A popular approach for adaptive MCMC algorithms is to view the scaling
%parameter as a random variable which we can sample during the course
%of the MCMC iterations. However, it can be slow to converge to the
%optimal value, and we may need an
%uninformative prior for the scaling parameter. Alternatively, the
%parameter may be randomly sampled at various points during the
%evolution of the chain. This results in some iterations which make
%larger global moves in the state space between modes in the target
%distribution, and some which make local moves. Algorithms of this type
%do not converge to an optimal value of the scaling parameter.

To adapt the scaling parameter $\beta$, we use a version of the
gradient ascent method modified for a stochastic function. Some more
sophisticated examples are described in
\cite{roberts2009examples,Ji2013adaptive,andrieu2006ergodicity}.
%Algorithm~\ref{tab:adapt} in Appendix~\ref{Sec:App}, is not
%presented as an optimal strategy but as a demonstration that the PAIS
%algorithm can be adaptively tuned effectively.

%From here on in, we use the random walk proposal unless otherwise stated,
%\begin{equation}y = x_{i-1}+\beta \omega_{i-1}, \qquad \omega_{i-1}
%  \sim \mathcal{N}(0,\Sigma) \qquad {\rm i.i.d.}
%\end{equation}
%where $x_{i-1}$ is our current state, $y$ is our proposed state and
%$\Sigma$ is a covariance operator which might come from the prior
%distribution. In the numerics section we will refer to $\beta$ as the
%scaling parameter.

%Using an adaptive strategy, we calculate a sequence
%$\{\beta^{(k)}\}_{k=1}$ which converges roughly to the optimal
%scaling parameter $\beta^*$, resulting in the optimal transition
%density $\chi$ for our MCMC algorithm. This optimal value will differ
%depending on the criterion we are optimising. We must choose some sequence
%of iterations, $\{n_k\}_{k=1}$, at which to update $\beta$, and due
%to the constraints on adaptive MCMC
%algorithms\cite{roberts2007coupling,roberts2009examples}, these $n_k$
%must grow exponentially further apart. This same adaptive approach
%can also be applied to the trivially parallelised MCMC algorithms to adaptively calculate their optimal scaling parameter $\beta^*$.

\begin{table}[!h]
\centering
\begin{algorithm}[H]
\DontPrintSemicolon
\BlankLine
	Define update times $\{n_k\}_k$.\;
	\For{$n=1,\dots,N$}{
		Compute normal PAIS steps.\;
		\If {$n \in \{n_k\}$}
		{
			Divide ensemble into two halves. Use these halves to estimate the gradient in $\neff$ at $\delta_k$.\;
			Update $\delta_k$ using gradient ascent,
				\[
					\delta_{k+1} = \delta_k + \gamma\nabla\neff.
				\]
			\label{algline:gradient_ascent}
		}
	}
\caption{The Adaptive PAIS Algorithm.\label{alg:adaptPAIS}}
\end{algorithm}
\end{table}
Our adaptive algorithm is given in Algorithm~\ref{alg:adaptPAIS}. We choose update times which, as suggested in Section~\ref{sec:ess} allow for a reasonable estimate of the effective sample size, but do not waste too many iterations. In Step~\ref{algline:gradient_ascent}, the gradient ascent parameter $\gamma$ may decrease over time, e.g. as a function of $n_k$.

% When implemented, there are a number of parameters which must be
% chosen to ensure efficient tuning of $\beta$. Firstly, the initial
% value of $\beta$ should be chosen so that the chains quickly explore the parameter space. However, if $\beta$ is chosen to be too
% large, then the method may become unstable. We also choose two
% iteration numbers at which we change our adaptive strategy. At
% iteration $N_\text{join}$ we resample using the full ensemble instead
% of using two subsets of the full ensemble. When using two subsets we
% are training the scaling parameter using the wrong ensemble size, this
% allows faster convergence but leads to a sub-optimal scaling
% parameter. Resampling with the full ensemble towards the end of the
% tuning allows fast convergence to the right order of magnitude,
% followed by fine tuning to the optimal scaling parameter. Finally we
% choose a time, $N_\text{stop}$, at which we stop updating our
% parameter.

\section{Approximate Multinomial Resampling}\label{sec:AMR}
Although the ETPF is optimal in terms of preserving statistics of the
sample, it can also become quite costly as the number of ensemble members
is increased. It is arguable that in the context of PAIS, we do not
require this degree of accuracy, and that a faster more approximate
method for resampling could be employed. One approach would be to use
the bootstrap resampler, which simply takes the $M$ ensemble members'
weights and constructs a multinomial distribution, from which $M$
samples are drawn. This is essentially the cheapest resampling
algorithm that one could construct. However it too has some
drawbacks. The algorithm is random, and as such it is possible for all
of the ensemble members in a particular region not to be sampled. This
could be particularly problematic when attempting to sample from a
multimodal distribution, where it might take a long time to find one
of the modes again. The bootstrap filter is also not guaranteed to
preserve the mean of the weighted sample, unlike the ETPF.

Ideally, we would like to use a resampling algorithm which is not
prohibitively costly for moderately or large sized ensembles,
which preserves the mean of the samples, and which makes it much
harder for the new samples to forget a significant region in the
density. This motivates the following algorithm, which we refer to as
approximate multinomial resampling (AMR).

Instead of sampling $M$ times from an $M$-dimensional multinomial
distribution as is the case with the bootstrap algorithm, we sample
once each from $M$ different multinomials. Suppose that we have $M$
samples $y_n$ with weights $w_n$. The multinomial sampled from in the
bootstrap filter has a vector of probabilities given by:
\begin{equation*}
\frac{1}{\sum w_n} [w_1,w_2,\ldots,w_M] = \bar{\bf w},
\end{equation*}
with associated states $y_n$.
We wish to find $M$ vectors $\{{\bf p}_1,{\bf p}_2,\ldots,{\bf p}_M\}
\subset \mathbb{R}^M_{\ge 0}$
such that  $\frac{1}{M} \sum {\bf p}_i = \bar{\bf w}$. The AMR is then
given by a sample from each of the multinomials defined by the vectors
${\bf p}_i = [p_{i,1},p_{i,2},\ldots,p_{i,M}]$ with associated states ${\bf y}_i$. Alternatively, as with the ETPF, a deterministic sample
can be chosen by picking each sample to be equal to the mean value of
each of these multinomial distributions, i.e. each new sample
$\hat{x}_i$ is given by:
\begin{equation}
\hat{x}_i = \sum p_{i,j} x_j, \qquad i \in \{1,2,\ldots,M\}.
\end{equation}

The resulting sample has several properties which are advantageous in
the context of being used with the PAIS algorithm. Firstly, we have
effectively chopped up the multinomial distribution used in the
bootstrap filter into $M$ pieces, and we can guarantee that exactly
one sample will be taken from each section. This leads to a much
smaller chance of losing entire modes in the density, if each of the
sub-multinomials is picked in an appropriate fashion. Secondly, if we do not make a random sample for
each multinomial with probability vector $\bf p_i$ but instead take
the mean of the multinomial to be the sample, this algorithm preserves
the mean of the sample exactly. Lastly, as we will see shortly, this
algorithm is significantly less computationally intensive than the
ETPF.

There are of course infinitely many different ways that one could use
to split the original multinomial up into $M$ parts, some of which
will be far from optimal. The method that we have chosen is loosely
based on the idea of optimal transport. We search out states with the
largest weights, and choose a cluster around these points based on
the closest states geographically. This method is not optimal since
once most of the clusters have been selected the remaining states
may be spread across the parameter space.

\begin{table}[!h]
\centering
\begin{algorithm}[H]
\DontPrintSemicolon
\BlankLine
	$\b{z} = M\bar{\b{w}}$.\;
	\For {$i = 1,\dots, M$}
	{
		$J = \argmax_j z_j$.\;
		$p_{i,J} = \min\{1,z_J\}$.\;
		$z_J = z_J - p_{i,J}$.\;
		\While {$\sum_j p_{i,j} <1$}
		{
			$K = \argmin_{k \in \{k|z_k>0\}} \|y_J - y_k\|$.\;
			$p_{i,K} = \min\{1-\sum_j p_{i,j}, z_K\}$.\;
			$z_K = z_K - p_{i,K}$.\;
		}
		$x_i = \sum_k p_{i,k}y_k$.\;
	}
\caption{The approximate multinomial resampler (AMR).\label{alg:AMR}}
\end{algorithm}
\end{table}

Algorithm~\ref{alg:AMR} describes the basis of the algorithm with
deterministic resampling, using the means of each of the
sub-multinomials as the new samples. This resampler was designed with the aims of being
numerically cheaper than the ETPF, and more accurate than straight multinomial
resampling. Therefore we now present numerical examples which
demonstrate this.

\begin{figure}[htb]
\centering
\subfigure[Relative error in $\mathbb{E}(X)$]{\includegraphics[width=0.45\textwidth]{"figures/Resampler_EX"}}
\subfigure[Relative error in
$\mathbb{E}(X^2)$]{\includegraphics[width=0.45\textwidth]{"figures/Resampler_EX2"}}\\
\subfigure[Relative error in $\mathbb{E}(X^3)$]{\includegraphics[width=0.45\textwidth]{"figures/Resampler_EX3"}}
\subfigure[Cost of resamplers per significant figure]{\includegraphics[width=0.45\textwidth]{"figures/Resampler_speed"}}\\
\caption{Comparison of the performance between different resampling schemes. The example in Section~\ref{sec:problem 1} is implemented for this demonstration.}
\label{fig:AMR}
\end{figure}

To test the accuracy and speed of the three resamplers (ETPF,
bootstrap and AMR), we drew a sample of size $M$ from the proposal distribution
$\mathcal{N}(1,2)$. Importance weights were assigned, based on a
target distribution of $\mathcal{N}(2,3)$. The statistics of the
resampled outputs were compared with the original weighted samples. Figure \ref{fig:AMR} (a)-(c) show how the relative errors in the first
three moments of the samples changes with ensemble size $M$ for the three different
samplers. As expected, the AMR lies somewhere between the high
accuracy of the ETPF and the less accurate bootstrap
resampling. Note that only the error for the bootstrap multinomial
sampler is presented for the first moment since both the ETPF and the
AMR preserve the mean of the original weighted samples up to machine precision. Figure \ref{fig:AMR} (d) shows how the computational cost,
measured in seconds, scales with the ensemble size for the three
different methods. These results demonstrate that the AMR behaves how we wish, and
importantly ensures that exactly one sample of the output will lie in
each region with weights up to $\frac{1}{M}$ of the total.

We will use the AMR in the numerics in Section \ref{sec:chem_conv}
where we have chosen to use a larger ensemble size. We do not claim
that the AMR is the optimal choice within PAIS, but it does have
favourable features, and demonstrates how different choices of
resampler can affect the speed and accuracy of the PAIS algorithm.

\section{Consistency of PAIS}\label{sec:consistency}

As outlined in~\cite{martino2015adaptive} consistency of population AIS algorithms can be considered in two different cases. In the first case we fix the number of iterations $N < \infty$, but allow the population size to grow to inifinity $M\rightarrow\infty$. In the second case we hold the population size fixed $M<\infty$, and allow infinitely many iterations $N\rightarrow\infty$.

In case one, from standard importance sampling results we know that for an iteration $n$, as $M\rightarrow\infty$, we obtain a consistent estimator for any statistic of interest,
\[
	\hat{\phi}(\Theta) \approx \frac{1}{\hat{Z}}\sum\limits_{i=1}^M \! \frac{1}{M}w_i\phi(\theta_i) \rightarrow \phi(\Theta),
\]
where the normalisation constant, $\hat{Z} = \frac{1}{M}\sum_{i=1}^M \! w_i$, also converges to the true normalisation constant $Z$~\cite{robert2013monte}.

Case two is slightly more involved. Estimation of the normalisation constant $Z$ is biased, and so estimates of statistics are sums of independent but biased estimators. Since the estimators are independent, the proof of consistency of PAIS in this second limit can be approached in the same way as the PMC algorithm, where it has been shown that $\hat{Z}\rightarrow Z$~\cite{robert2013monte}. Since the normalisation constant is consistent, sums of the independent estimators are also consistent.

\section{Numerical Examples}\label{Sec:Num}

In this section we demonstrate convergence of the PAIS algorithm. We
also investigate the convergence properties of PAIS compared with
naively parallelised Metropolis-Hastings samplers. We assess the
performance of the PAIS algorithm using the relative $L^2$ error
defined in \eqref{eqn:L2_error}, as well as the relative error in the
first moment \eqref{eq:34567}.

In the one-dimensional examples which follow, we have optimised the
naively parallelised RWMH algorithm using the optimal acceptance rate
$\hat{\alpha} = 0.5$. This value differs from the theoretical
asymptotic value of 0.234 which applies in higher dimensions, but this
higher acceptance rate is commonly used for one dimensional Gaussian
posteriors~\cite{rosenthal2011optimal}. To find the optimal scaling
parameter we minimise the statistic
\[
	T_{\text{MH}}(\beta) = \left| \frac{N_{\text{acc}}(\beta)}{N_{\text{total}}} - \hat{\alpha} \right|,
\]
where $N_{\text{acc}}(\beta)$ is the number of accepted moves and
$N_{\text{total}}$ is the total number of samples produced. For the
PAIS algorithm, we maximise the effective sample size as
discussed in Section~\ref{sec:ess}.

\subsection{Sampling from a one dimensional Gaussian distribution}\label{sec:problem 1}

This first example shows how PAIS handles searching for the mode of a
Gaussian distribution when the initial state is a long way out in the
tails.

\subsubsection{Target distribution}

Consider the simple case of a linear observation operator $\G(x) = x$,
where the prior on $x$ and the observational noise follow centred
Gaussian distributions. Then, following \eqref{eqn:like}, the Gaussian
posterior has the form
\begin{equation}\label{eqn:Gaussian posterior}
	\text{law}(\mu_D) = \pi(x|D) \propto \exp\left(-\frac{1}{2}\big\|x - D\big\|^2_{\sigma^2} - \frac{1}{2}\big\|x\big\|^2_{\tau^2}\right),
\end{equation}
where $\sigma^2$ and $\tau^2$ are the variances of the
observational noise and prior distributions respectively. In the
numerics which follow, we choose $\tau^2 =0.01$ and $\sigma^2 = 0.01$,
and we observe $x_\text{ref}=4$ noisily such that
\[
D = \mathcal{G}(x) + \eta \sim \mathcal{N}(\mathcal{G}(x_\text{ref}),\sigma^2) = \mathcal{N}(4,0.01).
\]
These values result in a posterior density in which the vast majority
of the density is out in the tails of the
prior distribution. The Kullback-Leibler (KL)
divergence, which gives us a measure of how different the prior and
posterior are, is $D_{KL}(\mu_D || \mu_0) = 4.67$ for this
problem. A KL divergence of zero indicates that two distributions are
identical almost everywhere.

\subsubsection{Numerical implementation}\label{sec:Implementation P1}

In each of the following simulations, we perform three tasks. First we
calculate the optimal value of $\beta$ by optimising the statistics
described in Section~\ref{sec:statistics}. We then run the algorithms
with optimal parameters to calculate and compare the convergence
rates. Finally, we implement the adaptive algorithms described in
Section~\ref{sec:adapt} and compare the convergence rates of these
algorithms with the nonadaptive algorithms.

{\bf (1) Finding the optimal parameters}: To find the optimal
parameters we choose 32 values of $\beta$ evenly spaced on a log scale
in the interval $[10^{-5}, 2]$. We run the PAIS-RW and RWMH algorithms
for one million iterations, each with an ensemble size of $M=50$. We took
32 repeats of both algorithms and then used the geometric means of the
sample statistics to find the optimal parameters.

{\bf (2) Measuring convergence of nonadaptive algorithms}: We run the
algorithms in Section~\ref{sec:problem 1} for one million iterations,
again with $M=50$. The simulations are repeated 32 times using the
optimal parameters found in (1). The performance of the algorithms is
judged by the convergence of the relative $L^2$ error statistic in
\eqref{eqn:L2_error}.

{\bf (3) Measuring convergence of adaptive algorithms}: We run the
adaptive algorithms under the same conditions as the nonadaptive
algorithms, and again use the relative $L^2$ error to compare
efficiency. The adaptive algorithms are initialised with $\beta^{(1)}$ = 1.


\subsubsection{Optimal values of $\beta$}\label{sec:Optimal values P1}

\begin{figure}[htb]
\centering
\subfigure[RWMH algorithm.]{\includegraphics[width=0.45\textwidth]{"figures/G2_rwmho"}}
\subfigure[PAIS-RW algorithm.]{\includegraphics[width=0.45\textwidth]{"figures/G2_paiso"}}
\caption{Finding optimal values of $\beta$ for the example in Section~\ref{sec:problem 1}. The setup is as in Section~\ref{sec:Implementation P1}. Resampling is performed using the ETPF.}
\label{fig:P1 opt beta}
\end{figure}

\begin{table}[!htb]
    \begin{minipage}{.5\linewidth}
      \centering
        \begin{tabular}{|c|r|}
	\hline
	Statistic											& RWMH \\ \hline
	$\beta_{\text{L2}}^*$								& 2.1e-2 \\
	$\beta_{\%}^*$									& 1.5e-1 \\
	Acceptance Rate ($\beta_{\text{L2}}^*$)				& 9.0e-1 \\
	Acceptance Rate ($\beta_{\%}^*$)					& 5.0e-1 \\
	\hline
	\end{tabular}
    \end{minipage}%
    \begin{minipage}{.5\linewidth}
      \centering
        \begin{tabular}{|l|r|r|}
	\hline
	Statistic							& PAIS-RW \\ \hline
	$\beta_{\text{eff}}^*$				& 4.7e-2 \\
	$\beta_{\text{var}(w(y))}^*$		& 5.8e-2 \\
	$\beta_{\text{L2}}^*$				& 3.9e-2 \\
	\hline
	\end{tabular}
    \end{minipage}
	\vspace{1mm}
	\caption{Optimal values of $\beta$ summarised from Figure~\ref{fig:P1 opt beta}. Statistics calculated as described in Section~\ref{sec:statistics}. The values $\beta^*_{\text{L2}}$ and $\beta^*_{\%}$ are the optimal scaling parameters found by optimising the relative $L^2$ errors and acceptance rate respectively. Similarly $\beta_{\text{eff}}^*$ and $\beta_{\text{var}(w(y))}^*$ optimise the effective sample size and variance of the weights statistics.}
	\label{table:prob1 opt beta}
\end{table}

Figure~\ref{fig:P1 opt beta} (a) shows the two values of $\beta$ which
are optimal according to the acceptance rate and relative $L^2$ error
criteria for the RWMH algorithm. The smaller estimate comes from the
relative $L^2$ error, and the larger from the acceptance rate. The
results in Figure~\ref{fig:P1 opt beta} are summarised in
Table~\ref{table:prob1 opt beta}. Since in general we cannot calculate
the relative $L^2$ error, we must optimise the algorithm using the
acceptance rate. From the relative $L^2$ error curve we can see that
the minimum is very wide and despite the optimal values being very
different there is not a large difference in the convergence rate.

Figure~\ref{fig:P1 opt beta} (b) shows the effective sample size ratio
compared to the error analysis and the variance of the weights. The
relative $L^2$ error graph is noisy, but it is clear that the maximum
in the effective sample size and the minimum in the variance of the
weights are both close to the minimum in the relative $L^2$ error. Due
to this we say that the estimate of the effective sample size found by
averaging the statistic over each iteration is a good indicator for
the optimal scaling parameter.

\subsubsection{Convergence of RWMH vs PAIS-RW}

\begin{figure}[htb]
\centering
\subfigure[Relative $L^2$ error.]{\includegraphics[width=0.45\textwidth]{"figures/G2_l2"}}
\subfigure[Relative error in first moment.]{\includegraphics[width=0.45\textwidth]{"figures/G2_moments"}}
\caption{Error analysis for the (A)RWMH and (A)PAIS-RW algorithms. The setup is as in Section~\ref{sec:Implementation P1} (2, 3). Resampling is performed using the ETPF.}
\label{fig:MH1 L2}
\end{figure}

Figure~\ref{fig:MH1 L2} shows that the PAIS-RW algorithm converges to
the posterior distribution significantly faster than the RWMH
algorithm, in both $L^2$ error and relative error in the moments. A
description of the speed up attained by this algorithm is given in
Section~\ref{sec:calc_saving}.

Both adaptive algorithms are run with initial values of
$\beta=1$. Figure~\ref{fig:MH1 L2} shows that after an initial burn-in
period the APAIS-RW algorithm catches up to the PAIS-RW algorithm, and
by the end of the simulation window is matching its performance. The
ARWMH algorithm does not perform quite as well, this is possibly due
to the fact that $T_{\text{MH}}(\beta)$ is not smooth at the optimal
value making it difficult to minimise.

\subsubsection{Scaling of the PAIS algorithm with ensemble size}

Throughout this example, we use an ensemble size $M=50$, but it is
interesting to see how the PAIS algorithm scales when we increase the
ensemble size, and if there is some limit below which the algorithm
fails. We implement the problem in Section~\ref{sec:problem 1}, using
the RWMH and PAIS-RW algorithms with ensemble sizes in the interval $M
\in [1, 160]$.

\begin{figure}[h]
\begin{center}
\includegraphics[width=0.6\textwidth]{"figures/PAIS_saving"}
\caption{Ratio of PAIS-RW samples required to reach the same tolerance as the RWMH algorithm.}
\label{fig:PAIS_saving}
\end{center}
\end{figure}

Figure~\ref{fig:PAIS_saving} was produced using the method of finding
optimal $\beta$ described in Section~\ref{sec:Implementation P1} (1),
then running 32 repeats at each ensemble size. The convergence rates
are then found by regressing through the data. The graph is still very
noisy but demonstrates that increasing the ensemble size continues to
reduce the number of iterations required in comparison with naively
parallelised MH. The decreasing trend shows superlinear improvement of
PAIS with respect to ensemble size, in terms of the number of
iterations required, which is a demonstration of our belief that
parallelism of MCMC should give us added value over and above that
provided by naive parallelism. This decrease is due to the increasing
effective sample size shown in Figure~\ref{fig:neff-M} (b).

\subsection{Sampling from Bimodal Distributions}\label{sec:bimodal}

In this second example we investigate the behaviour of the PAIS algorithm
when applied to bimodal problems. MH methods can
struggle with multimodal problems, particularly where switches between
the modes are rare, resulting in incorrectly proportioned modes in the
histograms. This example demonstrates that the PAIS algorithm redistributes 
chains to new modes as they are found. This means that we expect the number of chains in a mode to be
approximately proportional to the probability density in that mode. As a result, reconstructed posteriors with disproportional modes, as is familiar with the MH algorithms, are not produced.

\subsubsection{Target Distribution} \label{sec:tar}

We look at an `easy' problem, $B_1$, which has a KL divergence of
0.880, and a `harder' problem, $B_2$, which has a KL divergence of
3.647. Problem $B_1$ has two modes which are not too far apart. In
$B_2$ we increase the distance between the two modes which has the
effect of increasing the expected number of iterations that it takes
for a MCMC chain to jump between modes. These posteriors are shown in Figure~\ref{fig:problem 3 posteriors}.

The following setup is the same for both problems. We consider a
non-linear observation operator $\G(x) = x^2$, and assign the prior
$x \sim \mu_0 = \N(0, \tau^2=0.25)$. We assume that a noisy reading,
$D$, is taken according to $D = \G(x_\text{ref}) + \varepsilon$, where
$\varepsilon \sim \mu_\varepsilon = \N(0, \sigma^2 = 0.1)$. This results
in the non-Gaussian posterior
\[
	\pi(x|D) \propto \exp\left(-\frac{1}{2\sigma^2}\|x^2 - D\|^2 - \frac{1}{2\tau^2}\|x\|^2\right).
\]
To create the `easy' problem we say that the true value of
$\G(x_\text{ref}) = 0.75$, and the `hard' problem is generated using
$\G(x_\text{ref}) = 2$. In the numerics which follow we draw noise from
$\mu_\varepsilon$ to generate our data point.

\begin{figure}[htpb]
\begin{center}
\includegraphics[width=\textwidth]{"figures/posteriors3"}
\caption{Posterior distributions for problems $B_1$ and $B_2$ as
  described in Section \ref{sec:tar}.}
\label{fig:problem 3 posteriors}
\end{center}
\end{figure}

\subsubsection{Calculating values of Optimal $\beta^*$}\label{sec:BM1_opt_beta}

Calculating the optimal values of the scaling parameters for this
problem is similar to the previous example; we check only the acceptance
rate to find the optimal values for RWMH and we use the effective
sample size to find the optimal values for PAIS-RW.
Table~\ref{table:BM_opt_beta} gives the optimal values of $\beta$ for both
problems. The subscript on $\beta$ refers to the criterion which has been optimised.

\begin{table}[!htb]
    \begin{minipage}{.5\linewidth}
      \centering
        \begin{tabular}{|l|r|r|}
	\hline
	Algorithm							& $\beta^*_{\text{acc}}$	& $\beta^*_{\text{eff}}$ \\ \hline
	RWMH								& 4.8e-1					& - \\
	PAIS-RW								& -						& 1.0e-1\\
	\hline
	\end{tabular}
    \end{minipage}%
    \begin{minipage}{.5\linewidth}
      \centering
        \begin{tabular}{|l|r|r|r|}
	\hline
	Algorithm							& $\beta^*_{\text{acc}}$	& $\beta^*_{\text{eff}}$	& $\beta^*_{\text{L2}}$ \\ \hline
	RWMH								& 2.3e-1					& - 						& 9.3e-1\\
	PAIS-RW								& -						& 5.1e-2 					& 1.3e-1\\
	\hline
	\end{tabular}
    \end{minipage}
	\vspace{1mm}
	\caption{Optimal values of $\beta$ for $B_1$ (left) and $B_2$ (right).}
	\label{table:BM_opt_beta}
\end{table}

It is relatively simple to find optimal scaling parameters for problem $B_1$.
These values are given in Table~\ref{table:BM_opt_beta} (left). However
problem $B_2$ is much harder as transitions between the modes are extremely
unlikely for the standard RWMH algorithm. This means that we need to consider the
convergence on two levels; we should consider the algorithm's ability
to find both the modes, and also whether it can sample them in the correct proportions.

To get correctly proportioned modes with the RWMH algorithm it is
important that the chains can transition between the modes frequently, which
means that $\beta$ must be large. However, this leads to a lower
acceptance rate, and so we sacrifice convergence locally. For this reason, the RWMH algorithm is very slow to
converge for problems of this type.

We can achieve these two regimes in RWMH by tuning $\beta$ using the acceptance rate for local convergence, and by $L^2$ error for global convergence. Similarly in PAIS-RW we can use the effective sample size for local convergence, and the $L^2$ error for global convergence.

From Table~\ref{table:BM_opt_beta} (right) we see that there is a large difference between the optimal value of $\beta$ for each regime in RWMH, and so will result in inefficient sampling. The PAIS-RW algorithm manages to sample the local detail and the large scale behaviour with similar values of $\beta^*$; a clear advantage to using this algorithm for this problem.

\subsubsection{Convergence of RWMH vs PAIS-RW}


\begin{figure}[htb]
\centering
\subfigure[Relative $L^2$ error.]{\includegraphics[width=0.45\textwidth]{"figures/BM1_L2"}}
\subfigure[Absolute error in first moment.]{\includegraphics[width=0.45\textwidth]{"figures/BM1_moments"}}
\caption{Convergence of the PAIS-RW and RWMH algorithms for Problem $B_1$. Set up described in Section~\ref{sec:Implementation P1}. Resampling is performed using the ETPF.}
\label{fig:BM1_L2}
\end{figure}

As in the Gaussian example we see a significant speed up with the
PAIS-RW algorithm for problem $B_1$. Figure~\ref{fig:BM1_L2} shows the adaptive and nonadaptive convergence rates. We can see that the adaptive algorithms compare closely with the
respective nonadaptive algorithms and the improvement PAIS offers remains significant.

\begin{figure}[htb]
\centering
\subfigure[Relative $L^2$ error using the optimal scaling parameters.]{\includegraphics[width=0.45\textwidth]{"figures/BM2_L2"}}
\subfigure[Comparison of the relative $L^2$ error of the algorithms optimised by $L^2$ minimisation with locally optimised PAIS-RW with a scout chain.]{\includegraphics[width=0.45\textwidth]{"figures/BM2_scout"}}
\caption{Convergence of the PAIS-RW and RWMH algorithms for Problem $B_2$. Set up described in Section~\ref{sec:Implementation P1}. Resampling is performed using the ETPF.}
\label{fig:BM2_L2}
\end{figure}

For $B_2$ the algorithms are run with the global optimal value of $\beta^*$, and with the local optimal value of $\beta^*$. Figure~\ref{fig:BM2_L2} (a) shows that the algorithms using the globally optimal $\beta^*$ convergence at the desired rate, whereas the algorithms optimised using the acceptance rate and effective sample size initially converge faster but at some point forget the location of one of the modes, causing the convergence to flatten out.

Using the PAIS algorithm we can minimise the impact of forgotten modes by constantly allowing the algorithm to search them out. Since we have parallel chains, we can run PAIS-RW with the majority of chains using the locally optimal value of $\beta$, and one or two chains with a larger scaling parameter. These chains with larger scaling parameters act both as `scouts' for new modes, and act to aid in the over-dispersal of the proposal distribution. Figure~\ref{fig:BM2_L2} (b) shows the results of using 49 chains with the local optimal scaling parameter, and one chain with ten times the local optimal scaling parameter. We see that modes are not forgotten and the algorithm converges with the improvement we see from PAIS-RW in the other problems. Other methods of mode searches are described in \cite{lan2013wormhole}.

The adaptive algorithm can just as easily be applied to the PAIS algorithm with the `scout' chains described in the previous paragraph. Since we need two equally sized sub-ensembles, we will use two groups of 24 ensembles with the same proposal distributions, and each group will also have a `scout' chain with a scaling parameter ten times that of the rest of the group.

\begin{figure}[htb]
\begin{center}
\includegraphics[width=0.6\textwidth]{"figures/BM2_AL2"}
\caption{Convergence of the relative $L^2$ error for problem $B_2$, comparing the globally optimised nonadaptive algorithms with the locally optimised adaptive algorithms. The setup is as described in Section~\ref{sec:Implementation P1} (3). Resampling is performed using the ETPF.}
\label{fig:BM2_AL2}
\end{center}
\end{figure}

Comparing the convergence of the adaptive algorithms against the
nonadaptive algorithms in Figure~\ref{fig:BM2_AL2} shows that the
algorithms behave as expected. The adaptive RWMH algorithm tuned using
the acceptance rate converges at the same rate as the $L^2$ optimised
algorithm until the scaling parameter gets small, and therefore
switches between the modes are rare, and the relative heights of the
modes are decided by the arbitrary proportion of chains which are in
each mode at this point. The adaptive PAIS-RW with scout chains tuned to the effective sample size converges at about the same rate as the locally optimised nonadaptive algorithm also with scouts.

\subsubsection{Calculating the Speed Up in Convergence}\label{sec:calc_saving}

\begin{figure}
\begin{center}
\includegraphics[width=0.8\textwidth]{"figures/calc_saving"}
\caption{Illustration of calculating the number of PAIS-pCNL iterations required to reach a tolerance of $~10^{-2.8}$ as a percentage of MH iterations. The relative $L^2$ error graphs are from the Gaussian problem in Section~\ref{sec:problem 1}.}
\label{fig:calc_saving}
\end{center}
\end{figure}

The graphs in the previous section clearly show that the PAIS-RW algorithm converges faster than the RWMH algorithm when both are parallelised with the same ensemble size. We can calculate the number of iterations required to achieve a particular tolerance level in our solution for each algorithm and compare these to calculate a percentage saving. In Figure~\ref{fig:calc_saving} we demonstrate our calculation of the savings. The constants $c_1$ and $c_2$ are found by regressing through the data with a fixed exponent of $-1/2$ excluding the initial data points where the graph has not finished burning in.

A summary of the percentage of iterations required using the PAIS algorithm compared with the respective Metropolis-Hastings algorithms is given in Table~\ref{table:calc_savings}. The blank entries correspond to occasions when either the MH algorithm or PAIS algorithm hasn't converged to the posterior distribution.

\begin{table}[!h]
\centering
\begin{tabular}{|l|r|r|r|}
\hline
		& Gaussian & $B_1$ & $B_2$ \\ \hline
	RWMH & 10\% & 32\% & 12.6\% (scout) \\
	pCN & - & 32\% & - \\
	MALA & 42\% & 36\% & 40\% \\
	pCNL & 66\% & 56\% & - \\ \hline
\end{tabular}
\caption{Iterations for the PAIS algorithms required to achieve a desired tolerance as a percentage of the number of iterations required by the respective MH algorithms. The pCN and pCNL proposal distributions are taken from~\cite{cotter2013mcmc}.}
\label{table:calc_savings}
\end{table}

\subsubsection{A Useful Property of the PAIS Algorithm for Multimodal Distributions}

\begin{figure}[!h]
\begin{center}
\includegraphics[width=\textwidth]{"figures/BM2_suction"}
\caption{This figure demonstrates the redistribution property of the PAIS algorithm. Initially there is one chain in the positive mode, and 49 chains in the negative mode.}
\label{fig:BM2_suction}
\end{center}
\end{figure}

The biggest issue for the Metropolis-Hastings algorithms when sampling
from a posterior such as the one in $B_2$ is that it is unlikely that
the correct ratio of chains will be maintained in each of the modes, and since there is no interaction between the chains, there is no way to remedy this problem. The PAIS algorithm tackles this problem with its resampling step. The algorithm uses its dynamic kernel to build up an approximation of the posterior at each iteration, and then compares this to the posterior distribution via the weights function. Any large discrepancy in the approximation will result in a large or small weight being assigned to the relevant chain, meaning the chain will either pull other chains towards it or be sucked towards a chain with a larger weight. In this way, the algorithm allows chains to `teleport' to regions of the posterior which are in need of more exploration. Figure~\ref{fig:BM2_suction} shows Problem $B_2$ with initially 1 chain in the positive mode, and 49 chains in the negative mode. It takes only a handful of iterations for the algorithm to balance out the chains into 25 chains in each mode. The chains switch modes without having to climb the energy gradient in the middle.

\subsection{Sampling from non-Gaussian bivariate distributions}\label{sec:chem}
%***some introduction to chemical systems and QSSA
In this section we apply the PAIS algorithm to a more complicated posterior distribution. The field of biochemical kinetics gives rise to multiscale stochastic problems which remain a challenge both theoretically and computationally. Biochemical reactions occur in single cells between a number of chemical populations and the rates of these reactions can vary on vastly different timescales. It is these reaction rates which we are interested in finding descriptions for.

It is often possible to isolate which reactions are occurring more
frequently (the fast reactions) and which are occurring less
frequently (the slow reactions). The quasi-steady-state assumption
(QSSA) is the assumption that the fast reactions converge in distribution
on a timescale which is negligible with respect to the rate of
occurrence of the slow reactions. This assumption allows us to
approximate the dynamics of the slowly changing quantities in the
system by assuming that the fast quantities are in equilibrium with
respect to the fast reactions in isolation. This kind of model
reduction can be used to approximate the likelihood in an inverse
problem where we wish to recover the reaction parameters in the
system.

Let us consider a simple example by introducing the following simple
chemical system involving two chemical species $S_1$ and $S_2$:
\begin{equation}\label{eqn:full_system}
	\emptyset \xrightarrow{k_1} S_1 \xleftrightarrows[k_3]{k_2} S_2 \xrightarrow{k_4} \emptyset.
\end{equation}
Each arrow represents a reaction from a reactant to a product, with
some rate constant $k_i$, and where the rates of the reactions are
assumed to follow mass action kinetics. We denote the concentration of
species $S_i$ by $X_i$. We assume that we are in a
parameter regime such that the reactions $S_1\rightarrow S_2$ and $S_2\rightarrow S_1$ occur
much much more frequently than the other reactions. Notice that both
chemical species are involved in fast reactions. However, the quantity
$\mathcal{S} = X_1 + X_2$ is conserved by both of the fast reactions,
and as such, this is the slowly changing quantity in this system.
The effective dynamics of $\mathcal{S}$ can be represented as follows.
\begin{equation}\label{eqn:QSSA_system}
	\emptyset \xrightarrow{k_1} \mathcal{S} \xrightarrow{\hat{k}_4} \emptyset.
\end{equation}
Here, the new reaction rate $\hat{k}_4$ is approximated through
application of the QSSA to be
\[
	\hat{k}_4(s) = \mathbb{E}\left[k_4X_2|\mathcal{S}=s\right] = \frac{k_2k_4\mathcal{S}}{k_2+k_3}.
\]
The value of $\mathbb{E}\left[k_4X_2|\mathcal{S}=s\right]$ is
approximated by finding the steady state of the ODE representing the fast subsystem of
reactions:
\[S_1 \xleftrightarrows[k_3]{k_2} S_2, \qquad X_1 + X_2 = s\]

If we assume that we know the rate constants $k_1$ and $k_4$, and
observe the system in \eqref{eqn:QSSA_system}, we indirectly
observe the rates $k_2$ and $k_3$ through the effective rate
$\hat{k}_4$ of the degradation of $\mathcal{S}$. Our
observations are uninformative about these reaction rates, as there
are surfaces in parameter space along which the effective rate
$\hat{k}_4$ is invariant, leading to a highly ill-posed inverse
problem. Making the assumption that the errors in our observations of
the value of $\mathcal{S}$ are Gamma distributed with some variance
$\sigma^2$, this results in a long and thin posterior distribution on
$k_2$ and $k_3$. This type of problem is notoriously difficult to
sample from using standard MH algorithms, as the algorithms quickly
find a point on this manifold on which $\hat{k}_4$ is invariant, but
exploration along its length is slow.

% solution to QSSA ode and Gamma noise assumption
\subsubsection{Target Distribution}\label{sec:345}

We now formalise the posterior of interest. We look for a distribution over the parameter $\mathbf{k} = (k_2, k_3)^T$, given that we know $k_1 = 100$ and $k_4=1$. We generate our data by making ten observations of the system at $t_i = 2, 4, \dots, 20$, here simulated by solving the full system, \eqref{eqn:full_system}, governed by the differential equations
\begin{align*}
	\frac{\text{d}S_1}{\text{d}t} &= k_1 - k_2X_1(t)+k_3X_2(t), \\
	\frac{\text{d}S_2}{\text{d}t} &= k_2X_1(t) - (k_3+k_4)X_2(t),
\end{align*}
with initial conditions $X_1(0)=X_2(0) = 0$, and parameter values $\mathbf{k} = (50, 100)^T$. We then add noise taken from a Gamma distribution with variance $\sigma^2 = 225$ and centred at each $\mathcal{S}(t_i)=X_1(t_i)+X_2(t_i)$.

In our modelling we use the QSSA to simplify this system of differential equations into the one dimensional system based on \eqref{eqn:QSSA_system},
\[
	\frac{\text{d}\mathcal{S}}{\text{d}t} = k_1 - \frac{k_2k_4}{k_2+k_3}\mathcal{S}(t).
\]
The observation operator, $\mathcal{G}: (\mathbf{k},t) \mapsto S(t)$, maps from parameter space onto population space at a time $t$. This means that for the $i$th observation we assume,
\[
	 D_i \sim \text{Gamma}(\alpha_i, \beta_i),
\]
where
\[
	 \alpha_i = \frac{\mathcal{G}(\mathbf{k}, t_i)^2}{\sigma^2},\  \beta_i = \frac{\mathcal{G}(\mathbf{k}, t_i)}{\sigma^2}, \quad i = 1, \dots, 10.
\]
These are parameters required to give us a distribution with mean
$\mathcal{G}(\mathbf{k}$ and variance $\sigma^2$.
We assign Gamma priors to $\mathbf{k}$ with mean $\alpha_0/\beta_0 =
75$ and variance $\alpha_0/\beta_0^2 = 100$ in both coordinates, resulting in the posterior
\[
	\pi(\mathbf{k}|\mathbf{D}) \propto \left[\prod\limits_{i=1}^{10} \text{Gamma}(D_i; \alpha_i, \beta_i)\right]\text{Gamma}(k_2; \alpha_0, \beta_0)\text{Gamma}(k_3; \alpha_0, \beta_0).
\]

\begin{figure}[htb]
\centering
\includegraphics[width=0.65\textwidth]{"figures/C1_posterior"}
\caption{The posterior distribution on the parameters $k_2$ and $k_3$
  given the data and priors described in Section \ref{sec:345}.}
\label{fig:345}
\end{figure}

Figure \ref{fig:345} presents a visualisation of the posterior
distribution for this problem, found by exhaustive MH simulation.

\subsubsection{Implementation}\label{sec:chem_implementation}
% explanation of l2 convergence measure
For this problem there is no analytic form for the normalisation
constant, and numerical methods implemented in MATLAB and Mathematica
have proven to be unreliable. To demonstrate the convergence of the
algorithm, we first run a MH algorithm for much longer than we
normally would ($8\times 10^{10}$ samples), and consider the histogram
produced to be a well-converged approximation of the true posterior
distribution. We then perform our usual simulations with the PAIS and
MH algorithms to compare the rate at which the histograms converge to
the posterior over the same mesh.

% modifications to algorithm + spikes
In this problem we modify our PAIS algorithm to use a mixture of Gamma
distributions in the proposal distribution instead of a Gaussian
mixture. The PAIS-Gamma and MH-Gamma algorithms use a Gamma
proposal distribution with mean centred at the previous state,
\[
	y \sim \text{Gamma}(\cdot;\alpha^*, \beta^*) \quad \text{where} \quad \frac{\alpha^*}{\beta^*} = x \ \text{and} \ \frac{\alpha^*}{(\beta^*)^2} = \beta^2.
\]

Similarly the PAIS-LA algorithm uses the Langevin proposal distribution
from the standard MALA algorithm with a perturbationperturbation taken from a Gamma
distribution,
\[
	y \sim \text{Gamma}(\cdot; \alpha^*, \beta^*), \quad \text{where} \quad \frac{\alpha^*}{\beta^*} = x+\frac{1}{2}\beta^2\nabla\log\pi(X) \ \text{and} \ \frac{\alpha^*}{(\beta^*)^2} = \beta^2.
\]
This ensures that we only propose positive
parameters. Having a heavier right tail in the proposal distribution
also means that the posterior is absolutely continuous with respect to
the proposal. When this is not the case, the weight function can tend
to zero or infinity when $k_i \rightarrow \infty$, which results in
poor, very spiky approximations of the tails of the posterior distribution leading to slow convergence.

% AMR and increase in sample size
We now significantly increase the ensemble size we are using from $M=50$ to $M=2500$. This allows us to build a better approximation of the posterior distribution for our proposals. Following the discussion in Section~\ref{sec:AMR}, it is clear that to keep the runtime of the resampler negligible compared with the calculation of the posterior we must switch from the ETPF resampler to the AMR algorithm.

% numerics.
\subsubsection{Convergence of PAIS-Gamma and PAIS-LA vs MH with Gamma proposals}\label{sec:chem_conv}

In the numerics which follow we have used the AMR algorithm to resample with an ensemble size of $M=2500$. The method otherwise remains the same as in previous sections. We perform test runs to find the optimal scaling parameters for both Gamma proposals and MALA-type proposals. We then calculate the convergence rates of the algorithms by producing 50 million samples from the posterior with each algorithm, and repeat the simulation 32 times.

\begin{figure}[htb]
\centering
\subfigure[Optimal values of the scaling parameter.]{\includegraphics[width=0.45\textwidth]{"figures/C1_ESS"}}
\subfigure[Relative $L^2$ error against number of iterations.]{\includegraphics[width=0.45\textwidth]{"figures/C1_L2"}}
\caption{Convergence of the PAIS-Gamma and PAIS-LA algorithms for the chemical system problem described in Section~\ref{sec:chem}. Implementation described in Sections~\ref{sec:chem_implementation} and \ref{sec:chem_conv}. Resampling is performed using the AMR scheme.}
\label{fig:C1_ESS}
\end{figure}

\begin{table}[!htb]
      \centering
        \begin{tabular}{|l|r|r|r|}
	\hline
	Algorithm	& MH-Gamma & PAIS-Gamma & PAIS-LA \\ \hline
	$\beta^*$	& 2.7e-0     & 9.4e-1    & 1.0e-0 \\
	\hline
	\end{tabular}
	\vspace{1mm}
	\caption{Optimal values of the scaling parameter. The MH algorithm is optimised using the acceptance rate, and the PAIS algorithms are optimised using the effective sample size.}
	\label{table:C1_opt_beta}
\end{table}

In Figure~\ref{fig:C1_ESS} (a) we see that the two PAIS algorithms have similar optimal values of the scaling parameter, also shown in Table~\ref{table:C1_opt_beta}. However, the MALA-type proposal achieves a higher effective sample size with the same ensemble size. Convergence after the first 50 million samples is shown in Figure~\ref{fig:C1_ESS} (b) and demonstrates that the PAIS algorithm converges faster than the MH algorithm. The slightly unstable convergence in the PAIS-Gamma algorithm is the effect of spikes appearing in the tails of the posterior distribution. This is also what has caused the slightly lower effective sample size.

\subsection{A Mixture Model}\label{sec:mixture}

The technique of mixture modelling employs well known parametric families to construct an approximating nonparametric distribution which may have a complex structure. Most commonly, Gaussian kernels are used since underlying properties in the data can often be assumed to follow a Gaussian distribution. An example would be if a practitioner were to measure the heights of one hundred adults, but failed to record their gender. The data could be considered as one population with two sub populations, male and female. The problem then might be to find the average height of adult males from the data. In this case, since height is often considered to follow a Gaussian distribution, it makes sense to model the population as a mixture of two univariate Gaussian distributions.

A well known problem in the Bayesian treatment of mixture modelling is that of identification, sometimes referred to as the label-switching phenomenon. The likelihood distribution for mixture models is invariant under permutations of the mixture labels. If a mixture has $n$ means and the point $(\mu_1, \dots, \mu_n)$ maximises the likelihood, then the likelihood will also be maximised by $(\mu_{\varphi(1)}, \dots, \mu_{\varphi(n)})$ for all permutations $\varphi(\cdot)$. This means that the number of modes in the posterior distribution is of order $\mathcal{O}(n!)$. As we have seen it can be hard for standard MCMC algorithms to obtain reliable inference for posterior distributions with a large number of modes, or even a small number of modes which are separated by a large distance.

\subsubsection{Target Distribution}\label{sec:mixture_target}

In particular we look at a Gaussian mixture where we assume that there are two subpopulations within the overall population. Since both subpopulations will be approximated by Gaussian distributions we have five parameters which we need to be estimated, two means $\mu_{1,2}$, two variances $\sigma^2_{1,2}$, and the probability, $p$, that an individual observation belongs to the first subpopulation. We have 100 data points, $D_i$, which we assume to be distributed according to
\[
	D_i \sim p\mathcal{N}(\mu_1, \sigma^2_1) + (1-p)\mathcal{N}(\mu_2, \sigma^2_2), \quad i = 1,\dots,100,
\]
where $p \in [0, 1]$, $\mu_{1,2} \in \mathbb{R}$ and $\sigma^2_{1,2} \in \mathbb{R}^+$. Due to the domains of these parameters and also some prior knowledge, we assign the priors
\[
	p \sim \text{Beta}(1,1), \quad \mu_{1,2} \sim \mathcal{N}(0, 4) \quad \text{and} \quad \sigma^2_{1,2} \sim \text{Gamma}(\alpha=2, \beta=1).
\]
If we collect these parameters in the vector $\theta = (p, \mu_1, \sigma^2_1, \mu_2, \sigma^2_2)^\top$, the resulting posterior distribution is
\begin{equation}\label{eq:mixture_posterior}
	\pi(\theta|D) \propto \prod\limits_{i=1}^{100} (p\mathcal{N}(D_i;\mu_1, \sigma^2_1) + (1-p)\mathcal{N}(D_i;\mu_2, \sigma^2_2))\prod\limits_{i=1}^5 \pi_0^i(\theta_i),
\end{equation}
where $\pi_0^i(\cdot)$ is the prior density function corresponding to $\theta_i$.

\begin{figure}[htb]
\centering
\includegraphics[width=\textwidth]{"figures/PAIS_best_posterior"}
\caption{The posterior distribution of $\theta$ as given in Equation~\eqref{eq:mixture_posterior} as found from 10 million samples from the PAIS algorithm. The main diagonal contains the marginal distributions of each $\theta_i$, and the lower triangular contours represents the correlation between pairs of parameters.}
\label{fig:mixture_posterior}
\end{figure}

Figure \ref{fig:mixture_posterior} presents a visualisation of the posterior distribution for this problem, created from a single PAIS simulation.

\subsubsection{Implementation}\label{sec:mixture_implementation}

We have no analytic form for the posterior distribution for this model, and MH cannot reliably sample from this type of bimodal distribution. We do however know that the probability density should be evenly divided between the two modes. This is due to the symmetric prior for $p$, and and the same priors being assigned to $\mu_1$ and $\mu_2$, and also to $\sigma^2_1$ and $\sigma^2_2$. To decide which mode a sample belongs to we define a plane which bisects the posterior so that each point on this plane lies exactly halfway between the two true solutions to the inverse problem i.e. the value of $\theta$ used to generate the data, and also the $\theta$ obtained by a relabelling of the parameters. Now that we can assign a sample to a particular mode, we can calculate the density in each mode by summing the weights associated to all samples in that mode,
\[
	\bar{w}_k = \sum\limits_{i=1}^N w_i\text{I}(X_i \in \text{Mode $k$}), \quad k = 1, 2,
\]
and the relative error in the amount of density in each mode is then
\[
	w_\text{error} = 2\left|\frac{\bar{w}_1}{\bar{w}_1+\bar{w}_2} - \frac{1}{2}\right|.
\]

Since the probability $p$ is constrained to lie in the interval $[0,1]$, and the variances must be positive, it can be wasteful to use Gaussian proposal distributions. For this example we make proposals by matching the proposal distribution to the prior, scaling the variance and centring the proposal at the previous value. So
\[
	q_p \sim \text{Beta}(\delta^{-2}p, \delta^{-2}(1-p)), \quad q_{\mu_{1,2}} \sim \mathcal{N}(\mu_{1,2}, 4\delta^2) \quad \text{and} \quad q_{\sigma^2_{1,2}} \sim \text{Gamma}(\alpha^*, \beta^*),
\]
where $\alpha^* = \sigma^2_{1,2}\beta^*$, $\beta^* = \sigma^2_{1,2}/2\delta^2$ and $\delta$ is a scaling parameter to be tuned. This means that our proposal distributions will not be a mixture of multivariate Gaussians, but independent mixtures of univariate Beta, Gamma and Gaussian distributions.

In the numerics which follow we have increased the ensemble size from $M=50$ to $M=500$ to compensate for the increase in dimension. We also use the AMR algorithm for the resampling step because of the reduced computational cost. The method otherwise remains the same as in previous examples. We perform test runs to find the optimal scaling parameters considering convergence to modes with equal density. We then calculate the convergence rates of the algorithms by producing 10 million samples from the posterior with each algorithm, and repeat the simulation 32 times.

% numerics.
\subsubsection{Convergence of MH vs PAIS}\label{sec:mixture_conv}

\begin{table}[!htb]
      \centering
        \begin{tabular}{|l|r|r|}
	\hline
	Algorithm	& MH & PAIS \\ \hline
	$\delta^*$	& 1.3e-1     & 2.3e-1 \\
	\hline
	\end{tabular}
	\vspace{1mm}
	\caption{Optimal values of the scaling parameter. The MH algorithm is optimised using the acceptance rate, and the PAIS algorithm is optimised using the effective sample size.}
	\label{table:mixture_opt_beta}
\end{table}

The optimal scaling parameters for the MH and PAIS algorithms with the proposal distributions described in Section~\ref{sec:mixture_implementation} are given in Table~\ref{table:mixture_opt_beta}.

\begin{figure}[htb]
\centering
\includegraphics[width=0.8\textwidth]{"figures/Mode_proportions"}
\caption{Convergence of the PAIS algorithm for the mixture model described in Section~\ref{sec:mixture}. Implementation described in Sections~\ref{sec:mixture_implementation} and \ref{sec:mixture_conv}. Resampling is performed using the AMR scheme.}
\label{fig:mixture_modes}
\end{figure}

Convergence of the relative error for the two algorithms is displayed in Figure~\ref{fig:mixture_modes}. PAIS converges at the expected $\mathcal{O}(1/\sqrt{N})$ rate, whereas the MH algorithm converges to locally smooth histograms but with the wrong proportion of samples in each mode. The relatively low value of the error for the MH example is due to the priors covering the sample space evenly, however since transitions are near impossible with a small value of the scaling parameter, this error will take a very long time to reduce. This problem was discussed in Section~\ref{sec:bimodal}.

\section{Discussion and Conclusions}\label{Sec:Conc} 

We have explored the application of parallelised MCMC algorithms in
low dimensional inverse problems. We have demonstrated numerically
that these algorithms converge faster than the analogous naively parallelised
Metropolis-Hastings algorithms. Further experimentation with the Metropolis
Adjusted Langevin Algorithm (MALA), preconditioned Crank-Nicolson (pCN),
preconditioned Crank-Nicolson Langevin (pCNL) and Hamiltonian
Monte Carlo (HMC) proposals has yielded similar results\cite{Paul}.

Importantly, we have compared the efficiency of our parallel scheme
with a naive parallelisation of serial methods. Thus our increase in
efficiency is over and above an $M$-fold increase, where $M$ is the
number of ensemble members used. Our approach
demonstrates a better-than-linear speed-up with the number of ensemble
members used. 

The PAIS has a number of favourable features, for example the
algorithm's ability to redistribute, through the resampling regime,
the ensemble members to regions which require more exploration. This allows the
method to be used to sample from complex multimodal distribution.

Another strength of the PAIS is that it can also be used with any MCMC
proposal. There are a growing number of increasing sophisticated MCMC
algorithms (HMC, Riemann manifold MCMC etc) which could be
incorporated into this framework, leading to even more efficient
algorithms, and this is another opportunity for future work. 

% One disadvantage of parallelised algorithms is that often different
% processors will complete their tasks in different amount of times, for
% a number of reasons, often due to communication between the
% processors. One approach which could be applied to the PAIS to avoid
% this is for each processor to immediately start a new iteration, only
% using the last values of the other processors that were communicated
% to it. This incomplete approach would still be valid, and could lead
% to more efficient use of the computer architecture.

One limitation of the PAIS approach as described above is that a
direct solver of the ETPF problem (such as FastEMD \cite{FastEMD}) has computational cost
$\mathcal{O}(M^3\log M)$, where $M$ is the number of particles in the
ensemble. As such, we introduced a more approximate resampler the
approximate multinomial resampler, which allows us to push the
approach to the limit
with much larger ensemble sizes. The PAIS framework is very flexible
in terms of being able to use any combination of proposal
distributions and resampling algorithms that one wishes.

%  A second limitation is associated
% with the problem of accurately approximating measures with empirical
% measures in high dimensional phase space. This is a well-known issue
% in filtering, where one possible solution is to ``localise'' the
% impact of observations, as rigorously analysed in
% \cite{rebeschini2013can}. An established technique in Ensemble Kalman
% filters, localisation has also been incorporated into the Ensemble
% Transform Particle Filter \cite{ChRe2015}. This issue could be
% addressed in the PAIS by using localisation when computing the
% transform, but keeping the unlocalised weights in the importance
% sampling for consistency.

\bibliographystyle{siam}
\bibliography{refs}

% \begin{appendix}
% \section{The adaptive PAIS algorithm}\label{Sec:App}
% \begin{table}[!h]
% \begin{mdframed}
% \begin{algorithmic}
% \STATE $x_j^{(0)} \sim \mu_0$, for $j \in L\cup U$, where $L = \{j\}_{j=1}^{M/2},\ U = \{j\}_{j=M/2+1}^M$.
% \STATE Choose $\beta^{(1)} \in (0, 2]$. Set $\beta_{L,U}^{(1)} = (1\pm 0.01)\beta^{(1)}\wedge 2$.
% \FOR{$i=1,2, \ldots, N$}
% \STATE $y_j^{(i)}~\sim~Q(x_j^{(i-1)},\beta_L^{(i)})$ for $j \in L$, and $y_j^{(i)}~\sim~Q(x_j^{(i-1)},\beta_U^{(i)})$ for $j \in U$.
% \STATE Calculate 
% \[
% 	w^{(i)}_j = \frac{\pi(y_j^{(i)})}{\nu(y_j^{(i)};X^{(i-1)})},
% \]
% where
% \[
% 	\nu(y;X) = \frac{1}{M}\sum_{j\in L} q(y;x_j,\beta_L)+\frac{1}{M}\sum_{j\in U} q(y;x_j,\beta_U).
% \]

% \IF{$i$ is in $\{n_k\}_{k=1}$}
% 	\STATE For $w_{kj} = w_j^{(k)}$, $S = n_k - n_{k-1}$,
% 	\STATE $T_L = SM(\sum_{k=i-S}^S\sum_{j\in L} w_{kj})^2/(\sum_{k=i-S}^S\sum_{j\in L} w_{kj}^2)$.
% 	\STATE $T_U = SM(\sum_{k=i-S}^S\sum_{j\in U} w_{kj})^2/(\sum_{k=i-S}^S\sum_{j\in U} w_{kj}^2)$.
% 	\STATE $\beta^{(i+1)} = \beta^{(i)} - \Delta t \displaystyle\frac{T_U - T_L}{\beta_U^{(i)}-\beta_L^{(i)}}$.
% \ELSE
% 	\STATE $\beta^{(i+1)} = \beta^{(i)}$.
% \ENDIF

% \IF{$i < N_\text{stop}$}
% 	\IF{$i < N_\text{join}$}
% 		\STATE Resample 
% 	\[
% 		(w_j^{(i)},y_j^{(i)})_{j\in L} \rightarrow (\frac{1}{M}, x_j^{(i)})_{j\in L}, \quad (w_j^{(i)},y_j^{(i)})_{j\in U} \rightarrow (\frac{1}{M}, x_j^{(i)})_{j\in U}.
% \]
% 	\ELSE
% 		\STATE Resample $(w^{(i)},Y^{(i)}) \rightarrow (\frac{1}{M}\mathbf{1}, X^{(i)})$.
% 	\ENDIF
% 	\STATE $\beta_{L,U}^{(i)} = \beta^{(i)} \pm 2\sqrt{2}\beta^{(i)}/\sqrt{NM} \wedge 2$.
% \ELSE
% 		\STATE Resample $(w^{(i)},Y^{(i)}) \rightarrow (\frac{1}{M}\mathbf{1}, X^{(i)})$.
% 		\STATE $\beta_L^{(i+1)} = \beta_U^{(i+1)} = \beta^{(i+1)}$.
% \ENDIF
% \ENDFOR
% \end{algorithmic}
% \end{mdframed}\caption{A pseudo-code representation of the adaptive
%   PAIS algorithm.}
% \label{tab:adapt}
% \end{table}
% \end{appendix}

\end{document}
