\documentclass{article}
\usepackage{graphicx}
\usepackage{amsfonts}
\usepackage{url}
\usepackage{pdfpages}
\usepackage{amsmath,amssymb}
\usepackage{enumerate}
\usepackage{color}
\usepackage{amsthm,amsmath}
\usepackage{bbm}
\usepackage{mhchem}

%for stacking below a sum or max...
\usepackage{mathtools}

\usepackage[colorlinks]{hyperref}

\newcommand{\comment}[2]{\vspace{0.6cm}{\bf Comment:} {\it #1.}

\vspace{0.3cm}{\bf Answer:} #2}

\begin{document}
\title{Response to Referee Comments: Parallel Adaptive Importance Sampling}
\maketitle
First of all, we would like to thank the referees and the associate editor for
their time in reading and commenting on our manuscript. In this document
we will address each of the points raised, and describe any relevant
changes that have been made to the paper. ****INSERT SUMMARY OF MAJOR CHANGES****

\section*{Referee 1 comments}

\comment{This is supposed to be a parallel algorithm and yet it's implemention is serial. Furthermore, the authors don't seem to be entirely sure that parallelization will work since they comment that it's effectiveness will depend on various factors. The title seems somehow misleading to me. Maybe if the "parallel" part of the title is removed and the discussion about parallelization is treated as a possibility instead of a critical part of the algorithm, then the document will be less confusing.}{Despite the fact that we felt that we had demonstrated the feasibility of the parallelisation of this method, we agree with the referee that we have not demonstrated this in the numerics. Furthermore, this algorithm delivers a speed-up even when run in serial. As such, we are happy to change the name of the algorithm to ``Ensemble Transport Adaptive Importance Sampling'' (ETAIS). This required some rewriting throughout the paper. In particular, we have toned down the emphasis on parallelisation, and put more emphasis on the fact that this algorithm also delivers speed-up for serial implementaions.}

\comment{Also, the comparison between serial PAIS and the naive parallel MCMC is strange if there is no parallelization.}{The naive parallel chains are run for long enough that they are equivalent to one long chain. We have added a discussion on this point to *****}

\comment{It will be interesting that the authors show the PAIS using AMS with the examples. They show the difference between the AMS and the ETPF but not within the PAIS.}{We assume that the referee meant the AMR? As the AE asked for a new example to replace the first two examples, we took this as an opportunity to compare ETAIS with the ETPF, AMR and bootstrap resamplers, which is now presented in the new first example. The numerics demonstrate that superior samplers lead to more stable and accurate sampling, with ETPF algorithm narrowly outperforming the AMR algorithm, with the bootstrap version following up in the rear.}

\comment{There is an error at the end of page 24.}{Many thanks for spotting this, this has now been corrected.*******}

\section*{Referee 2 comments}
\comment{The paper is ready for publication.}{Many thanks. We hope you agree that the additional changes have further improved the paper.}

\section*{Associate Editor comments}

\comment{The authors have made a substantial improvement over previous version of the paper. However, there are a number of points that I would like to see clarified before the paper is accepted for publication.\\
1) The posterior sometimes is called $\mu$ sometimes $\pi$ some times $\pi(x|D)$ etc. In fact, there is a confusion in Alg. 1 in p.6 with this.}{We have carefully gone through the paper and clarified where appropriate whether we are referring the posterior measure $\mu$ or its density $\pi$, and unified the notation as much as possible.}

\comment{2) Choosing the proposal (kernel) in a MH is all the main and commonly the only problem in MCMC. PAIS also needs a proposal of exactly the same kind. Why can we expect that within PAIS this will be easier? An indeed, PAIS will perform better that a naively parallelized MH, both using the same proposal. How PAIS saves some of the problems in finding good proposals?}{As discussed in the paper, there are many possible choices for the proposal kernel around each of the ensemble members. The only thing that we require to ensure convergence of an AIS algorithm, is the absolute continuity of the kernel with respect to the posterior. However, to ensure stability of the algorithm, we also require that the tails of the proposal distributions are at least as fat as the target. In practice, this means that picking kernels which are of the same type as the prior is usually a good idea. The ETAIS methodology does not, however, greatly benefit from the use of more informed and expensive proposals, such as are seen in the standard M-H framework, where gradient information can be used to improve proposals (MALA, HMC, etc), since the information about the whole target is represented by the ensemble. As mentioned in the paper, we have conducted similar studies with MALA proposals, and although we sometimes observe a slightly quicker burn-in period, overall the performance is largely the similar to that of a RW proposal, at twice the cost. We have added a detailed discussion around this to ******}

\comment{3) It might be trivially to do so, but the authors only briefly mention a prove for the convergence of their algorithm. In fact, is it ok to use a dependent proposal in importance sampling? ... no regularity conditions are needed? Why Alg. 3 works? resampling "introduces error" ... is this relevant? does the error cascades down in each iteration? In the examples the PAIS seem to work ok, but, do we need any regularity condition? Please make your convergence claims more explicit and take more time to explain them. In particular, do we have the correct invariant distribution?}{This algorithm belongs to a family of methods for which convergence has been proven in the literature, and as such, we have referenced this proof. We have clarified this a little further in *******}

\comment{4) There are too many examples. The univariate gaussian is not very informative and the univariate bimodal, is not either since both modes are of the same scale. I would rather see these two examples turned into one single one with a 2D mixture of two separated gaussians with contrasting scales (variance covariance matrices).}{At the request of the AE, we have removed the first two examples, and replaced them with a single example, which is a two dimensional bimodal density with differing scales in the covariances of the two modes. We have also taken this opportunity to demonstrate the differences in results when using the different resamplers discussed in the paper. We have also included a comparison with a standard bootstrap resampler. The new numerics demonstrate the improvement in stability and convergence of the method with the use of more advanced resampling methods. The remaining examples say **** and BLAH and so we're keeping them.*****}

\comment{5) In the examples the authors include very ad hoc strategies like the 'scout' chain and specially the very ad hoc strategy described in section 7.4.2 (!). The authors, in my opinion, do not really discuss the implementation of the PAIS algorithm but a case-by-case ad hoc version of it. This is clearly unacceptable.}{We strongly disagree that the approaches used are in any way ``ad hoc''. As per the discussion following the AE's 2nd comment, it is important that the overall proposal has tails that are as fat as those of the target. The ``scout'' chain, as also used in [ref], ensures two things. 1, it allows for larger moves into unexplored regions of the state-space, and it also fattens the tails of the proposal distribution. As for the proposal kernels used in 7.4.2, these are chosen simply because the proposal distribution must only be supported by [0,1]. We have gone into more details regarding this and explaining the choices, and how users might make their own choices for different scenarios, in *********.}

\comment{6) Please clarify the first phrase of section 7.4.2, which, as far as I see, is simply twice wrong. There is a (very simple!) analytic version of the posterior AND with the right proposal MH can sample from it (!).}{We have amended the statement about the analytic form of the posterior. We are not aware of any M-H methods which can reliably sample from multimodal densities with well-separated modes. HMC trajectories can be hand-tuned to make transitions between modes, but only with information regarding the separation. Ensemble-based methods appear to us to be the only candidate for reliable sampling from such distributions without such a hand-tuning.}

\comment{7) Although the chaotic Lorenz 93 ODE is used, it is used in such a short time scale that the posterior is quite well behaved (unimodal and fairly gaussian and only some correlation is seen between to parameters). Since the priors are normal, this regime suggest a linear part of the FM. The authors should attempt something more challenging, by running the system into a highly nonlinear regime. Otherwise, why consider this example?}{The reason for considering the example was that the AE had previously asked us to produce an example in the regime where we suggest that we get the biggest win using our proposed method, i.e. those where the likelihood evaluations have significant cost due to requirement of the solution of a differential equation. In this example, we have demonstrated a significant speed-up over the equivalent M-H algorithm. We do understand however the AE's desire for a more challenging inverse problem in itself, and so we have created a new example that *********. The new example still demonstrates a speed-up of ********.}

\comment{8) Do not use an explicit Euler solver, use any higher order, off the shelve solver.}{Many of the off-the-shelf solvers are just as inappropriate to use for the approximation of the solution of the Lorenz '63 equations, since they are chaotic, and as such \emph{any} numerical error is quickly amplified through the dynamics. Moreover, the focus of this paper is not on these applications, but in the statistical algorithm. In this example, we are simply looking for a non-linear mapping such that we can demonstrate the speed-up that is gained from using ETAIS as opposed to standard M-H sampling, and we believe that a simple Euler approximation provides such a mapping.}
\end{document}
